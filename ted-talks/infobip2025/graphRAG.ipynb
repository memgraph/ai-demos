{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequirements\n",
    "\n",
    "- Install python\n",
    "- Install docker and docker compose\n",
    "- Create new python environment with VS Code or install manually\n",
    "    ```\n",
    "    python -m venv .venv\n",
    "    ```\n",
    "- Install required packages from 3rd block.\n",
    "- Copy the openai API Key from following url: https://justpaste.it/infobip-graphrag-workshop and paste it into the OpenAI object in 4th block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG in Memgraph\n",
    "\n",
    "In this tutorial, we will build a Q&A application that utilizes **GraphRAG** powered by Memgraph and\n",
    "OpenAI's LLM.\n",
    "\n",
    "In this example, we will use **vector search** on node embeddings to find\n",
    "semantically relevant data. After relevant data is located, the structured data\n",
    "will be extracted from the graph and passed to LLM to ground it for a more accurate answer to our question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries\n",
    "Installs the required Python libraries: Neo4j driver (to connect with Memgraph) and OpenAI SDK (to interact with the LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install neo4j==5.28.2                   # for driver and connection to Memgraph\n",
    "%pip install openai==1.107.3                 # for access to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI and GraphDatabase\n",
    "Initializes connections to OpenAI (for LLM calls) and Memgraph (for graph storage/queries), setting up the core services used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "client = OpenAI(api_key=\"Replace with the key from: https://justpaste.it/infobip-graphrag-workshop\")\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an vector index\n",
    "Creates a vector index on the Speaker node’s speaker_embedding property in Memgraph, ensuring fast similarity search. Runs once to avoid duplicate index creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_speaker_index():\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"SHOW INDEX INFO;\")\n",
    "        existing_indexes = [\n",
    "            (record[\"label\"], record[\"property\"]) for record in result\n",
    "        ]\n",
    "\n",
    "        if (\"Speaker\", \"speaker_embedding\") not in existing_indexes:\n",
    "            session.run(\n",
    "                \"\"\"CREATE VECTOR INDEX speaker_embedding ON :Speaker(speaker_embedding) WITH CONFIG {\"dimension\": 1536, \"capacity\": 1000};\"\"\"\n",
    "            )\n",
    "            print(\"Index 'speaker_embedding' created.\")\n",
    "        else:\n",
    "            print(\"Index 'speaker_embedding' already exists.\")\n",
    "\n",
    "setup_speaker_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embedding vector\n",
    "Generates an embedding vector for a given text using OpenAI’s text-embedding-3-small model, returning embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed existing data\n",
    "Fetches all Speaker nodes, generates embeddings for their names via OpenAI, and stores those embeddings back into Memgraph for use in similarity-based queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_speakers_data():\n",
    "    with driver.session() as session:\n",
    "        # Fetch all Speaker nodes\n",
    "        result = session.run(\"MATCH (s:Speaker) RETURN id(s) AS id, s.name AS name\")\n",
    "        \n",
    "        for record in result:\n",
    "            speaker_id = record[\"id\"]\n",
    "            text = record[\"name\"]\n",
    "\n",
    "            embedding = get_embedding(text)\n",
    "\n",
    "            # Store embedding in Memgraph\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MATCH (s:Speaker) WHERE id(s) = $id\n",
    "                SET s.speaker_embedding = $embedding\n",
    "                \"\"\",\n",
    "                id=speaker_id,\n",
    "                embedding=embedding\n",
    "            )\n",
    "\n",
    "embed_speakers_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector search\n",
    "Runs a vector similarity search in Memgraph against the specified index using the given query vector, returning the most similar nodes (e.g., closest speakers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query_vector):\n",
    "    query = f\"\"\"\n",
    "    CALL vector_search.search(\"speaker_embedding\", 1, {query_vector}) YIELD * RETURN *;\n",
    "    \"\"\"\n",
    "\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        memgraph_response = [record.data() for record in result]\n",
    "        return memgraph_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the relevant data\n",
    "\n",
    "Once we have the pivot node, we can retrieve the relevant structured\n",
    "data around it. The goal is to enrich the prompt given to the LLM with relevant data to provide a more accurate answer. The most straightforward approach to expanding the knowledge is to **perform multiple hops** starting from the pivot node.  \n",
    "\n",
    "The `get_relevant_data` is the function that fetches the data around the pivot node, a specified number\n",
    "of `hops` away from the pivot node.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_data(node, hops):\n",
    "    paths = []\n",
    "\n",
    "    name = node[\"node\"][\"name\"]\n",
    "    with driver.session() as session:\n",
    "        query = f'MATCH path=((n:Speaker {{name: \"{name}\"}})-[r*..{hops}]->(m)) RETURN path'\n",
    "        result = session.run(query)\n",
    "\n",
    "        for record in result:\n",
    "            path_data = []\n",
    "            for segment in record[\"path\"]:\n",
    "                # Process start node without 'embedding'\n",
    "                start_node_data = {\n",
    "                    k: v\n",
    "                    for k, v in segment.start_node.items()\n",
    "                    if k not in [\"speaker_embedding\"]\n",
    "                }\n",
    "\n",
    "                # Process relationship data\n",
    "                relationship_data = {\n",
    "                    \"type\": segment.type,\n",
    "                    \"properties\": segment.get(\"properties\", {}),\n",
    "                }\n",
    "\n",
    "                # Process end node without 'embedding'\n",
    "                end_node_data = {\n",
    "                    k: v\n",
    "                    for k, v in segment.end_node.items()\n",
    "                    if k not in [\"speaker_embedding\"]\n",
    "                }\n",
    "\n",
    "                # Add to path_data as a tuple (start_node, relationship, end_node)\n",
    "                path_data.append(\n",
    "                    (start_node_data, relationship_data, end_node_data)\n",
    "                )\n",
    "\n",
    "            paths.append(path_data)\n",
    "\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the **prompting logic** for the RAG pipeline\n",
    "\n",
    "* `RAG_prompt` → Builds a prompt that combines the user’s question with graph-expanded data, instructing the model to answer only from that context.\n",
    "* `question_prompt` → Builds a prompt to extract the **key entity** (e.g., person or talk) from a user question.\n",
    "* `get_response` → Sends the prompt to OpenAI (`gpt-5-mini`) and returns the model’s response text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_prompt(question, relevance_expansion_data):\n",
    "    prompt = f\"\"\"\n",
    "    I will provide you with a question and a set of data obtained through a relevance expansion process in a graph database. \n",
    "    The relevance expansion process finds nodes connected to a target node within a specified number of hops and includes \n",
    "    the relationships between these nodes.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Relevance Expansion Data:\n",
    "    {relevance_expansion_data}\n",
    "\n",
    "    Based on the provided data, please answer the question, make sure to base your answers only based on the provided data. \n",
    "    Add a context on what data did you base your answer on. If you can't find the right answer, \n",
    "    politely suggest to ask another question and explain that you don't have enough context.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def question_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    Your task is to extract the key subject entity from a question. \n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Output **only** the most relevant entity (the person or the talk) needed to answer the question — nothing else.\n",
    "\n",
    "    Examples:\n",
    "    Question: \"Where did Clay Shirky talk?\"\n",
    "    Key entity: Clay Shirky\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "async def get_response(client, prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end RAG pipeline\n",
    "\n",
    "Extracts the key entity from the user’s question.\n",
    "\n",
    "Generates an embedding for that entity.\n",
    "\n",
    "Finds the most similar node in Memgraph via vector search.\n",
    "\n",
    "Expands the graph around that node (2 hops) to gather relevant context.\n",
    "\n",
    "Builds a RAG-style prompt with the question + context.\n",
    "\n",
    "Queries the LLM to produce the final grounded answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question  (feel free to change the question) \n",
    "question = 'Which talks did Clay Shirky held?'\n",
    "\n",
    "async def ask_question(question):\n",
    "\n",
    "    # Key information from the question \n",
    "    prompt = question_prompt(question)\n",
    "    response = await get_response(client, prompt)\n",
    "    print(response)\n",
    "\n",
    "    # Compute the embedding for the key information\n",
    "    question_embedding = get_embedding(response)\n",
    "\n",
    "    # Find the most similar node to the question embedding\n",
    "    node = vector_search(question_embedding)\n",
    "    if node:\n",
    "        print(\"\\n\\nThe most similar node is:\")\n",
    "        print(node[0])\n",
    "\n",
    "    # Get the relevant data based on the most similar node\n",
    "    relevant_data = get_relevant_data(node[0], hops=1)\n",
    "\n",
    "    # Show the relevant data\n",
    "    print(\"\\n\\nThe relevant data is:\")\n",
    "    print(relevant_data)\n",
    "\n",
    "    # LLM answers the question based on the relevant data\n",
    "    prompt = RAG_prompt(question, relevant_data)\n",
    "    response = await get_response(client, prompt)\n",
    "    print(\"\\n\\nThe response is:\")\n",
    "    print(response)\n",
    "\n",
    "await ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Identify the speakers from a specific TED Talk event.\n",
    "\n",
    "The question we want to answer is:\n",
    "> Who spoke at TED2009 event?\n",
    "\n",
    "Use what you’ve just learned, and feel free to edit, copy, adapt or remove the code provided above to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_event_index():\n",
    "    raise NotImplementedError(\"Replace this with your code\")\n",
    "\n",
    "setup_event_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_event_data():\n",
    "    raise NotImplementedError(\"Replace this with your code\")\n",
    "\n",
    "embed_event_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_event(query_vector):\n",
    "    raise NotImplementedError(\"Replace this with your code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who spoke at TED2009 event?'\n",
    "\n",
    "async def ask_question(question):\n",
    "    raise NotImplementedError(\"Replace this with your code\")\n",
    "\n",
    "await ask_question(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
