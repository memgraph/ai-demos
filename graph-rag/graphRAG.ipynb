{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG in Memgraph\n",
    "\n",
    "In this tutorial, we will build a Q&A application that utilizes **GraphRAG** powered by Memgraph and\n",
    "OpenAI's GPT LLM. This example is based on a portion of a fixed Game of Thrones dataset,\n",
    "which will be enriched with unstructured data to create a knowledge graph. \n",
    "\n",
    "In this example, we will use **vector search** on node embeddings to find\n",
    "semantically relevant data. After relevant data is located, the structured data\n",
    "will be extracted from the graph and passed to LLM to ground it for a more accurate answer to our question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To begin with this tutorial, you will need Docker, Python and an OpenAI API key.\n",
    "If you prefer using a local LLM, adapt this setup with a few small tweaks when defining the model. \n",
    "\n",
    "First, start **Memgraph** with the [vector search](https://memgraph.com/docs/querying/vector-search) feature included by running the following command in your preferred terminal: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```sh\n",
    "docker run -p 7687:7687 -p 7444:7444 memgraph/memgraph-mage:1.22-memgraph-2.22 \\\n",
    "  --log-level=TRACE \\\n",
    "  --also-log-to-stderr \\\n",
    "  --experimental-enabled=vector-search \\\n",
    "  --experimental-config='{\"vector-search\": {\"got_index\": {\"label\": \"Entity\", \"property\": \"embedding\", \"dimension\": 384, \"capacity\": 10000, \"metric\": \"cos\"}}}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Memgraph supports vector\n",
    "search starting from version 2.22. The `--experimental-config` flag defines on what\n",
    "`label` and `property` the vector will be stored. This is currently predefined\n",
    "during the startup, and it cannot be changed dynamically. \n",
    "\n",
    "Once Memgraph is running in the background, load the initial Game\n",
    "of Thrones dataset by running the following commands in the terminal: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "\n",
    "```sh\n",
    "cd memgraph-graphRAG\n",
    "cat ./data/memgraph-export-got.cypherl | docker run -i memgraph/mgconsole --host=host.docker.internal\n",
    "echo \"MATCH (n), ()-[r]->() RETURN count(DISTINCT n) AS node_count, count(DISTINCT r) AS relationship_count;\" | docker run -i memgraph/mgconsole --host=host.docker.internal\n",
    "```\n",
    "```plaintext\n",
    "+--------------------+--------------------+\n",
    "| node_count         | relationship_count |\n",
    "+--------------------+--------------------+\n",
    "| 2677               | 11967              |\n",
    "+--------------------+--------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node count and relationship count should highlight if everything was loaded properly. If you are running this on Linux, make sure to change the `--host` to `localhost`\n",
    "\n",
    "After the dataset is ingested, install the necessary prerequisites to run the demo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j==5.27.0\n",
      "  Using cached neo4j-5.27.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pytz in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from neo4j==5.27.0) (2024.1)\n",
      "Using cached neo4j-5.27.0-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: neo4j\n",
      "Successfully installed neo4j-5.27.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence-transformers==3.3.1\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers==3.3.1)\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tqdm (from sentence-transformers==3.3.1)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers==3.3.1)\n",
      "  Using cached torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting scikit-learn (from sentence-transformers==3.3.1)\n",
      "  Downloading scikit_learn-1.6.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers==3.3.1)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-macosx_14_0_x86_64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers==3.3.1)\n",
      "  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers==3.3.1)\n",
      "  Downloading pillow-11.0.0-cp310-cp310-macosx_10_10_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (4.11.0)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers==3.3.1)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers==3.3.1)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.3.1) (3.1.4)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3.1)\n",
      "  Downloading numpy-2.2.0-cp310-cp310-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3.1)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3.1)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers==3.3.1)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers==3.3.1)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.3.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3.1) (2024.8.30)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers==3.3.1)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Using cached torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "Downloading pillow-11.0.0-cp310-cp310-macosx_10_10_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.0-cp310-cp310-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-macosx_14_0_x86_64.whl (25.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.5/25.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.0-cp310-cp310-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl (287 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-macosx_10_12_x86_64.whl (392 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, safetensors, regex, Pillow, numpy, networkx, joblib, fsspec, filelock, torch, scipy, huggingface-hub, tokenizers, scikit-learn, transformers, sentence-transformers\n",
      "Successfully installed Pillow-11.0.0 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.5 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.0 regex-2024.11.6 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.14.1 sentence-transformers-3.3.1 sympy-1.13.3 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.2.2 tqdm-4.67.1 transformers-4.47.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting openai==1.56.2\n",
      "  Using cached openai-1.56.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from openai==1.56.2) (4.6.2)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.56.2)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from openai==1.56.2) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.56.2)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-macosx_10_12_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.56.2)\n",
      "  Using cached pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: sniffio in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from openai==1.56.2) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from openai==1.56.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from openai==1.56.2) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.56.2) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.56.2) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.56.2) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.56.2) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.56.2) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai==1.56.2)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=1.9.0->openai==1.56.2)\n",
      "  Downloading pydantic_core-2.27.1-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai==1.56.2)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Using cached openai-1.56.2-py3-none-any.whl (389 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.2-cp310-cp310-macosx_10_12_x86_64.whl (303 kB)\n",
      "Using cached pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp310-cp310-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: typing-extensions, jiter, distro, annotated-types, pydantic-core, pydantic, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.8.2 openai-1.56.2 pydantic-2.10.3 pydantic-core-2.27.1 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-dotenv==1.0.1\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install neo4j==5.27.0                   # for driver and connection to Memgraph\n",
    "%pip install sentence-transformers==3.3.1    # for calculating sentence embeddings\n",
    "%pip install openai==1.56.2                  # for access to LLM\n",
    "%pip install python-dotenv==1.0.1            # for environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich knowledge graph with the embeddings \n",
    "\n",
    "In GraphRAG, you are not writing actual Cypher queries, but you are\n",
    "asking the questions about your domain knowledge graph in plain English. From your question, you want to\n",
    "retrieve relevant parts of the knowledge graph. \n",
    "\n",
    "To achieve this, you can encode the semantic meaning into the graph so you can locate\n",
    "the semantically similar parts of the graph based on the question you have provided.\n",
    "\n",
    "There are a several approaches to consider: **embedding the node labels and\n",
    "properties**, **embedding the triplets related to a node** (node-relationship-node) or **embedding specific paths\n",
    "you can take from node**. \n",
    "\n",
    "The decision on which approach to take depends on your needs and resources. Encoding a larger amount of information requires a larger vector (higher dimension), which can be costly in terms of memory and performance.\n",
    "\n",
    "Embedding triplets or paths is a better choice than embedding node labels and properties if you're dealing with longer questions. That happens because triplets or paths encode more information, ensuring that parts of the graph semantically similar to the question are located more accurately. That is quite important to consider for the reliability of the Q&A applications because if semantic search misses relevant parts of the graph, the LLM will not be able to answer the question correctly. \n",
    "\n",
    "To illustrate a basic example, here is a function that calculates embeddings based on the node labels and properties: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(driver, model):\n",
    "    with driver.session() as session:\n",
    "        # Retrieve all nodes\n",
    "        result = session.run(\"MATCH (n) RETURN n\")\n",
    "\n",
    "        for record in result:\n",
    "            node = record[\"n\"]\n",
    "            # Check if the node already has an embedding\n",
    "            if \"embedding\" in node:\n",
    "                print(\"Embedding already exists\")\n",
    "                return\n",
    "\n",
    "            # Combine node labels and properties into a single string\n",
    "            node_data = (\n",
    "            \" \".join(node.labels)\n",
    "            + \" \"\n",
    "            + \" \".join(f\"{k}: {v}\" for k, v in node.items())\n",
    "            )\n",
    "\n",
    "            # Compute the embedding for the node\n",
    "            node_embedding = model.encode(node_data)\n",
    "\n",
    "            # Store the embedding back into the node\n",
    "            session.run(\n",
    "            f\"MATCH (n) WHERE id(n) = {node.element_id} SET n.embedding = {node_embedding.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Set the label to Entity for all nodes\n",
    "        session.run(\"MATCH (n) SET n:Entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the function above, if we have a node `:Character {name:\"Viserys\n",
    "Targaryen\"}` in the graph, the encoded embedding will include the label\n",
    "`:Character` and the property `name: Viserys Targaryen`. After that embedding is\n",
    "calculated it will be stored into the `embedding` property of the node. \n",
    "\n",
    "Asking the question `Who is Viserys Targaryen?` will yield a very similar\n",
    "embedding, allowing you to locate that node in the graph. However, if you ask a\n",
    "longer question like, `To whom was Viserys Targaryen Loyal in season 1 of Game\n",
    "of Thrones?`, there is a chance that this question might not locate the `Viserys\n",
    "Targaryen` node in the graph due to its length and complexity. \n",
    "\n",
    "\n",
    "In the end of the `compute_embeddings` function, each node will get a `Entity`\n",
    "label so you can perform the vector search on top of all the nodes in the\n",
    "database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the relevant part of the graph\n",
    "\n",
    "Once embeddings are calculated in your graph, you can perform a search based on\n",
    "these embeddings by using a **vector search**. \n",
    "\n",
    "The goal is to find the most similar node to your question and to\n",
    "extract the relevant knowledge from it. The function `find_most_similar_node` compares the question's\n",
    "embedding to the embeddings stored on the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_node(driver, question_embedding):\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Perform the vector search on all nodes based on the question embedding\n",
    "        result = session.run(\n",
    "            f\"CALL vector_search.search('got_index', 10, {question_embedding.tolist()}) YIELD * RETURN *;\"\n",
    "        )\n",
    "        nodes_data = []\n",
    "        \n",
    "        # Retrieve all similar nodes and print them\n",
    "        for record in result:\n",
    "            node = record[\"node\"]\n",
    "            properties = {k: v for k, v in node.items() if k != \"embedding\"}\n",
    "            node_data = {\n",
    "                \"distance\": record[\"distance\"],\n",
    "                \"id\": node.element_id,\n",
    "                \"labels\": list(node.labels),\n",
    "                \"properties\": properties,\n",
    "            }\n",
    "            nodes_data.append(node_data)\n",
    "        print(\"All similar nodes:\")\n",
    "        for node in nodes_data:\n",
    "            print(node)\n",
    "\n",
    "        # Return the most similar node\n",
    "        return nodes_data[0] if nodes_data else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the similarity between the question embeddings and node embeddings, we\n",
    "get the most similar node. \n",
    "\n",
    "This node serves as a **pivot point** from which we can\n",
    "pull **relevant data**. For example, if we are searching for information about\n",
    "`Viserys Targaryen`, we would pull data surrounding that node, making it our\n",
    "pivot node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the relevant data\n",
    "\n",
    "Once we have the pivot node, we can retrieve the relevant structured\n",
    "data around it. The goal is to enrich the prompt given to the LLM with relevant data to provide a more accurate answer. The most straightforward approach to expanding the knowledge is to **perform multiple hops** starting from the pivot node.  \n",
    "\n",
    "The `get_relevant_data` is the function that fetches the data around the pivot node, a specified number\n",
    "of `hops` away from the pivot node.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_data(driver, node, hops):\n",
    "    with driver.session() as session:\n",
    "        # Retrieve the paths from the node to other nodes that are 'hops' away\n",
    "        query = (\n",
    "            f\"MATCH path=((n)-[r*..{hops}]-(m)) WHERE id(n) = {node['id']} RETURN path LIMIT 100\"\n",
    "        )\n",
    "        result = session.run(query)\n",
    "\n",
    "        paths = []\n",
    "        for record in result:\n",
    "            path_data = []\n",
    "            for segment in record[\"path\"]:\n",
    "\n",
    "                # Process start node without 'embedding' property\n",
    "                start_node_data = {\n",
    "                    k: v for k, v in segment.start_node.items() if k != \"embedding\"\n",
    "                }\n",
    "\n",
    "                # Process relationship data\n",
    "                relationship_data = {\n",
    "                    \"type\": segment.type,\n",
    "                    \"properties\": segment.get(\"properties\", {}),\n",
    "                }\n",
    "\n",
    "                # Process end node without 'embedding' property\n",
    "                end_node_data = {\n",
    "                    k: v for k, v in segment.end_node.items() if k != \"embedding\"\n",
    "                }\n",
    "\n",
    "                # Add to path_data as a tuple (start_node, relationship, end_node)\n",
    "                path_data.append((start_node_data, relationship_data, end_node_data))\n",
    "\n",
    "            paths.append(path_data)\n",
    "\n",
    "        # Return all paths\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We drop the `embedding` property from the nodes to avoid overloading the LLM's limited context with non-relevant data. \n",
    "\n",
    "Limit the depth of hops or the number of returned results to further shorten the data provided to the LLM.\n",
    "Also, be careful with supernodes, as hopping to a supernode can produce too much information and exceed the LLM context window size. \n",
    "\n",
    "## Defining the prompts\n",
    "\n",
    "For the LLM to understand its task, it needs specific prompts. The `RAG_prompt`\n",
    "describes how the LLM should answer the question, while the `question_prompt` is\n",
    "optimized for calculating question embeddings by extracting only the key pieces\n",
    "of information to improve embedding accuracy. For example, if you ask, `Who is\n",
    "Viserys Targaryen?`, only the `Viserys Targaryen` will be extracted from the\n",
    "question. Ultimately, the LLM will receive the full question back in the\n",
    "`RAG_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_prompt(question, relevance_expansion_data):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI language model. I will provide you with a question and a set of data obtained through a relevance expansion process in a graph database. The relevance expansion process finds nodes connected to a target node within a specified number of hops and includes the relationships between these nodes.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Relevance Expansion Data:\n",
    "    {relevance_expansion_data}\n",
    "\n",
    "    Based on the provided data, please answer the question, make sure to base your answers only based on the provided data. Add a context on what data did you base your answer on.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def question_prompt(question):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI language model. I will provide you with a question. \n",
    "    Extract the key information from the question. The key information is important information that is required to answer the question.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    The output format should be like this: \n",
    "    Key Information: [key information 1], [key information 2], ...\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "async def get_response(client, prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the GraphRAG\n",
    "\n",
    "We implemented the `ask_question` function to perform a GraphRAG query. \n",
    "Before we do that, we need to:\n",
    "\n",
    "1. Connect to the database \n",
    "2. Load the .env file with the `OPENAI_API_KEY=` defined\n",
    "3. Compute and store the node embeddings\n",
    "4. Define the question\n",
    "\n",
    "Then, the `ask_question` function takes the question and: \n",
    "\n",
    "1. Computes the question embedding based on key information \n",
    "2. Performs the vector search to find the most semantically similar node\n",
    "3. Gets the relevant data that is a few hops away from the pivot node\n",
    "4. Asks LLM the question with the relevant data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/p_/0vk_3w7d61zg1kd_b6b0ncc40000gn/T/ipykernel_7592/3839561946.py\", line 2, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/sentence_transformers/__init__.py\", line 9, in <module>\n",
      "    from sentence_transformers.backend import (\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/sentence_transformers/backend.py\", line 11, in <module>\n",
      "    from sentence_transformers.util import disable_datasets_caching, is_datasets_available\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/sentence_transformers/util.py\", line 17, in <module>\n",
      "    import torch\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/katelatte/opt/anaconda3/envs/new/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the SentenceTransformer model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mcompute_embeddings\u001b[0;34m(driver, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m node_data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(node\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute the embedding for the node\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m node_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Store the embedding back into the node\u001b[39;00m\n\u001b[1;32m     24\u001b[0m session\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMATCH (n) WHERE id(n) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39melement_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m SET n.embedding = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_embedding\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:674\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m    673\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    676\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfrom_numpy(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m all_embeddings]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:674\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    672\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m    673\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    676\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfrom_numpy(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m all_embeddings]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "## Get all dependencies \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import neo4j\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "\n",
    "# Create a driver\n",
    "driver = neo4j.GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"\", \"\"))\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "compute_embeddings(driver, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question  (feel free to change the question) \n",
    "question = \"In which episode was Viserys Targaryen killed?\"\n",
    "\n",
    "def ask_question(driver, question, model):\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Load .env file\n",
    "    load_dotenv()\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    # Key information from the question \n",
    "    prompt = question_prompt(question)\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "    print(response)\n",
    "    key_information = response.split(\"Key Information: \")[1].strip()\n",
    "\n",
    "    # Compute the embedding for the key information\n",
    "    question_embedding = model.encode(key_information)\n",
    "\n",
    "    # Find the most similar node to the question embedding\n",
    "    node = find_most_similar_node(driver, question_embedding)\n",
    "    if node:\n",
    "        print(\"The most similar node is:\")\n",
    "        print(node)\n",
    "\n",
    "    # Get the relevant data based on the most similar node\n",
    "    relevant_data = get_relevant_data(driver, node, hops=2)\n",
    "\n",
    "    # Show the relevant data\n",
    "    print(\"The relevant data is:\")\n",
    "    print(relevant_data)\n",
    "\n",
    "    # LLM answers the question based on the relevant data\n",
    "    prompt = RAG_prompt(question, relevant_data)\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "    print(\"The response is:\")\n",
    "    print(response)\n",
    "\n",
    "ask_question(driver, question, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example questions\n",
    "\n",
    "Here are a few examples of precalculated questions and answers: \n",
    "\n",
    "**To whom was Viserys Targaryen loyal to?**\n",
    "\n",
    ">Based on the provided data, Viserys Targaryen was loyal to House Targaryen.\n",
    ">This information is derived from the relationships indicating that Viserys\n",
    ">Targaryen was loyal to House Targaryen and the connections between them\n",
    "\n",
    "**Who killed Viserys Targaryen in Game of Thrones?**\n",
    "    \n",
    ">Based on the provided relevance expansion data, Khal Drogo killed Viserys\n",
    ">Targaryen in \"Game of Thrones.\" This information is inferred from the\n",
    ">relationship where Khal Drogo is linked to Viserys Targaryen with the action of\n",
    ">being \"KILLED\" by Khal Drogo. The data does not show any other character\n",
    ">directly killing Viserys Targaryen.\n",
    "\n",
    "**What was the weapon used to kill Viserys Targaryen in Game of Thrones?**\n",
    "    \n",
    ">Based on the provided data, the weapon used to kill Viserys Targaryen in Game\n",
    ">of Thrones was not explicitly mentioned. The data only shows that Khal Drogo\n",
    ">was involved in the killing of Viserys Targaryen. There is no specific mention\n",
    ">of the weapon used in the relevance expansion data. Therefore, I do not have\n",
    ">enough information to answer the question about the weapon used to kill Viserys\n",
    ">Targaryen.\n",
    "\n",
    "**This response is wrong**. A method, not a weapon, is mentioned, but LLM didn't catch the context due to the different naming. \n",
    "\n",
    "**Who betrayed Viserys Targaryen in Game of Thrones?**\n",
    "   \n",
    ">Based on the provided data, Khal Drogo betrayed Viserys Targaryen in Game of\n",
    ">Thrones by killing him. This conclusion is drawn from the relationship between\n",
    ">Khal Drogo and Viserys Targaryen where it is stated that Khal Drogo killed\n",
    ">Viserys Targaryen.\n",
    "\n",
    "This response is not necessarily accurate because it's based on the killing relationship, while betrayal could have different consequences. \n",
    "\n",
    "Let's expand this knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding the knowledge\n",
    "\n",
    "Let's say that now we want to expand our existing knowledge graph with\n",
    "additional information to enrich the dataset, provide more context and retrieve\n",
    "more relevant data. \n",
    "\n",
    "In this example, we will take **unstructured data**, such as the\n",
    "character description summary provided below, extract entities from that\n",
    "summary, generate triplets to build the knowledge graph create queries and\n",
    "eventually execute those queries in Memgraph to incorporate with the existing\n",
    "graph. \n",
    "\n",
    "\n",
    "This highlights the possibility of loading an unstructured data into the Memgraph. \n",
    "\n",
    "Here is an example of unstructured data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text summary for processing\n",
    "summary = \"\"\"\n",
    "    Viserys Targaryen is the last living son of the former king, Aerys II Targaryen (the 'Mad King').\n",
    "    As one of the last known Targaryen heirs, Viserys Targaryen is obsessed with reclaiming the Iron Throne and \n",
    "    restoring his family’s rule over Westeros. Ambitious and arrogant, he often treats his younger sister, Daenerys Targaryen, \n",
    "    as a pawn, seeing her only as a means to gain power. His ruthless ambition leads him to make a marriage alliance with \n",
    "    Khal Drogo, a powerful Dothraki warlord, hoping Khal Drogo will give him the army he needs. \n",
    "    However, Viserys Targaryen’s impatience and disrespect toward the Dothraki culture lead to his downfall;\n",
    "    he is ultimately killed by Khal Drogo in a brutal display of 'a crown for a king' – having molten gold poured over his head. \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity extraction\n",
    "\n",
    "The first step in the process is to extract entities from the summary using\n",
    "[SpaCy’s LLM](https://spacy.io/usage/large-language-models).\n",
    "\n",
    "To begin, we need to install SpaCy and the specific model we wll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%pip install spacy_llm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are extracting entities from the text, that is, preprocessing the data before\n",
    "sending it to the GPT model, to get more accurate and relevant results. By\n",
    "using SpaCy, we can identify key entities such as characters and locations\n",
    "for a better understanding of the semantics in the text.\n",
    "\n",
    "This is useful because SpaCy is specifically trained to recognize\n",
    "linguistic patterns and relationships in text, which helps to isolate and\n",
    "highlight the most important pieces of information. By preprocessing the text\n",
    "this way, we ensure that the GPT model receives a more structured input, helps\n",
    "reduce noise and irrelevant data, leading to more precise and context-aware\n",
    "outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy_llm.util import assemble\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Split document into sentences\n",
    "def split_document_sent(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "def process_text(text, nlp, verbose=False):\n",
    "    doc = nlp(text)\n",
    "    if verbose:\n",
    "        print(f\"Text: {doc.text}\")\n",
    "        print(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Pipeline to run entity extraction\n",
    "def extract_entities(text, nlp, verbose=False):\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "\n",
    "    sentences = split_document_sent(text, nlp)\n",
    "    for sent in sentences:\n",
    "        doc = process_text(sent, nlp, verbose)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        # Store processed data for each sentence\n",
    "        processed_data.append({\"text\": doc.text, \"entities\": entities})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open(\"processed_data.json\", \"w\") as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate queries\n",
    "\n",
    "After the spacyLLM has pre-processed the entities, the data is passed to the GPT model to generate structured data consisting of nodes and relationships. From that, we generate the Cypher queries which will be executed in Memgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cypher_queries(nodes, relationships):\n",
    "    queries = []\n",
    "\n",
    "    # Create nodes\n",
    "    for node in nodes:\n",
    "        query = f\"\"\"\n",
    "        MERGE (n:{node['type']}:Entity {{name: '{node['name']}'}}) \n",
    "        ON CREATE SET n.id={node['id']} \n",
    "        ON MATCH SET n.id={node['id']}\n",
    "        \"\"\"\n",
    "        queries.append(query)\n",
    "\n",
    "    # Create relationships\n",
    "    for rel in relationships:\n",
    "        query = f\"MATCH (a {{id: {rel['source']}}}), (b {{id: {rel['target']}}}) \" \\\n",
    "                f\"CREATE (a)-[:{rel['relationship']}]->(b)\"\n",
    "        queries.append(query)\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching the graph\n",
    "\n",
    "The `enrich_graph_data` function will merge new knowledge into the graph by doing the following:\n",
    "\n",
    "1. Extracting the entities with SpacyLLM into JSON\n",
    "2. Creating nodes and relationships based on extracted entities with GPT model\n",
    "3. Loading data into Memgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def enrich_graph_data(driver, summary):\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    load_dotenv()\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Sample text summary for processing\n",
    "    summary = \"\"\"\n",
    "        Viserys Targaryen is the last living son of the former king, Aerys II Targaryen (the 'Mad King').\n",
    "        As one of the last known Targaryen heirs, Viserys Targaryen is obsessed with reclaiming the Iron Throne and \n",
    "        restoring his family’s rule over Westeros. Ambitious and arrogant, he often treats his younger sister, Daenerys Targaryen, \n",
    "        as a pawn, seeing her only as a means to gain power. His ruthless ambition leads him to make a marriage alliance with \n",
    "        Khal Drogo, a powerful Dothraki warlord, hoping Khal Drogo will give him the army he needs. \n",
    "        However, Viserys Targaryen’s impatience and disrespect toward the Dothraki culture lead to his downfall;\n",
    "        he is ultimately killed by Khal Drogo in a brutal display of 'a crown for a king' – having molten gold poured over his head. \n",
    "    \"\"\"\n",
    "\n",
    "    extract_entities(summary, nlp)\n",
    "\n",
    "    # Load processed data from JSON\n",
    "    json_path = Path(\"processed_data.json\")\n",
    "    with open(json_path, \"r\") as f:\n",
    "        processed_data = json.load(f)\n",
    "\n",
    "    # Prepare nodes and relationships\n",
    "    nodes = []\n",
    "    relationships = []\n",
    "\n",
    "    # Formulate a prompt for GPT-4\n",
    "    prompt = (\n",
    "        \"Extract entities and relationships from the following JSON data. For each entry in data['entities'], \"\n",
    "        \"create a 'node' dictionary with fields 'id' (unique identifier), 'name' (entity text), and 'type' (entity label). \"\n",
    "        \"For entities that have meaningful connections, define 'relationships' as dictionaries with 'source' (source node id), \"\n",
    "        \"'target' (target node id), and 'relationship' (type of connection). Create max 30 nodes, format relationships in the format of capital letters and _ inbetween words and format the entire response in the JSON output containing only variables nodes and relationships without any text inbetween. Use following labels for nodes: Character, Title, Location, House, Death, Event, Allegiance and following relationship types: HAPPENED_IN, SIBLING_OF, PARENT_OF, MARRIED_TO, HEALED_BY, RULES, KILLED, LOYAL_TO, BETRAYED_BY. Make sure the entire JSON file fits in the output\"\n",
    "        \"JSON data:\\n\"\n",
    "        f\"{json.dumps(processed_data)}\"\n",
    "    )\n",
    "\n",
    "    response = asyncio.run(get_response(client, prompt))\n",
    "\n",
    "    structured_data = json.loads(response)  # Assuming GPT-4 outputs structured JSON\n",
    "\n",
    "    # Populate nodes and relationships lists\n",
    "    nodes.extend(structured_data.get(\"nodes\", []))\n",
    "    relationships.extend(structured_data.get(\"relationships\", []))\n",
    "\n",
    "    cypher_queries = generate_cypher_queries(nodes, relationships)\n",
    "    with driver.session() as session:\n",
    "        for query in cypher_queries:\n",
    "            try:\n",
    "                session.run(query)\n",
    "                print(f\"Executed query: {query}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing query: {query}. Error: {e}\")\n",
    "\n",
    "\n",
    "enrich_graph_data(driver, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knowledge graph now has additional knowledge, that is being enriched from unstructured text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before enriching the graph with more data, we had the following situation:\n",
    "\n",
    "**Who betrayed Viserys Targaryen in Game of Thrones?**\n",
    "\n",
    ">Based on the provided data, Khal Drogo betrayed Viserys Targaryen in Game of\n",
    ">Thrones by killing him. This conclusion is drawn from the relationship between\n",
    ">Khal Drogo and Viserys Targaryen where it is stated that Khal Drogo killed\n",
    ">Viserys Targaryen.\n",
    "\n",
    "This response is not necessarily accurate because it's based on the killing relationship, while betrayal could have different consequences. So, in a sense, LLM made that conclusion based on the wrong relationship. \n",
    "\n",
    "If we now, with the enriched graph, ask the same question, it yields a different, correct answer: \n",
    "\n",
    "**Who betrayed Viserys Targaryen in Game of Thrones?**\n",
    "\n",
    ">Based on the provided data, Khal Drogo betrayed Viserys Targaryen in Game of\n",
    ">Thrones. This conclusion is derived from the relationship between Viserys\n",
    ">Targaryen and Khal Drogo, where Khal Drogo is connected to Viserys Targaryen\n",
    ">through the 'BETRAYED_BY' relationship, indicating that Khal Drogo betrayed\n",
    ">Viserys Targaryen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test it yourself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question  (feel free to change the question) \n",
    "question = \"Who betrayed Viserys Targaryen in Game of Thrones?\"\n",
    "\n",
    "ask_question(driver, question, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
