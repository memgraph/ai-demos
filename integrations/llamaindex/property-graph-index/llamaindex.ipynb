{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memgraph Property Graph Index\n",
    "\n",
    "In this example, we're using Memgraph's integration with\n",
    "[LlamaIndex](https://www.llamaindex.ai/) to build a **Property Graph Index**\n",
    "from a Paul Graham essay and use it to retrieve structured insights.  \n",
    "\n",
    "- We start by **downloading** the essay and preparing the text for processing.  \n",
    "- Next, we **connect to Memgraph**, a graph database, to store and manage our\n",
    "  structured data.  \n",
    "- We then **create a Property Graph Index**, transforming the unstructured text\n",
    "  into a structured graph using OpenAIâ€™s embedding and language models.  \n",
    "- Finally, we **query the graph** using both a retriever and a query engine to\n",
    "  extract meaningful relationships from the text.  \n",
    "\n",
    "This notebook demonstrates how to turn raw text into a **queryable knowledge\n",
    "graph**, making it easier to analyze and retrieve insights from documents.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Run Memgraph**\n",
    "Before running Memgraph, ensure you have [Docker](https://www.docker.com/)\n",
    "running in the background. The quickest way to try out Memgraph Platform\n",
    "(Memgraph database + MAGE library + Memgraph Lab) for the first time is running\n",
    "the following command:\n",
    "\n",
    "For Linux/macOS:\n",
    "`curl https://install.memgraph.com | sh`\n",
    "\n",
    "For Windows:\n",
    "`iwr https://windows.memgraph.com | iex`\n",
    "\n",
    "From here, you can check Memgraph's visual tool, [Memgraph\n",
    "Lab](https://memgraph.com/docs/data-visualization) on the\n",
    "`http://localhost:3000/` or the [desktop version](https://memgraph.com/download)\n",
    "of the app.\n",
    "\n",
    "2. **Install necessary dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-graph-stores-memgraph python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create vector index in Memgraph on the `__Entity__` label and `embedding`\n",
    "   property. LlamaIndex creates embeddings and uses Memgraph's [vector\n",
    "   search](https://memgraph.com/docs/querying/vector-search) for more accurate\n",
    "   retrieval.\n",
    "\n",
    "`CREATE VECTOR INDEX entity ON :__Entity__(embedding) WITH CONFIG {\"dimension\": 1536, \"capacity\": 1000};`\n",
    "\n",
    "\n",
    "## Create the script\n",
    "\n",
    "First, let's create an `.env` file that contains your OpenAI API key:\n",
    "\n",
    "`OPENAI_API_KEY=sk-proj-...`\n",
    "\n",
    "We then load our `.env` file and set the LLM model we want to use. In this\n",
    "example, we're using OpenAI's gpt-4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the data directory and download the Paul Graham essay we'll be\n",
    "using as the input data for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/paul_graham/\", exist_ok=True)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
    "output_path = \"data/paul_graham/paul_graham_essay.txt\"\n",
    "urllib.request.urlretrieve(url, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Using LlamaIndex's `SimpleDirectoryReader`, we're loading the textual data from\n",
    "our defined data directory. This prepares the document for further processing,\n",
    "such as indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(content)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Memgraph\n",
    "\n",
    "To establish a connection with Memgraph, set up the `MemgraphPropertyGraphStore`\n",
    "class by providing your database credentials. You need to specify the username,\n",
    "password, and connection URL (e.g., `bolt://localhost:7687`).  \n",
    "\n",
    "Once initialized, this `graph_store` object will allow you to interact with\n",
    "Memgraph and store or retrieve graph-based data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.memgraph import MemgraphPropertyGraphStore\n",
    "\n",
    "username = \"\"  # Enter your Memgraph username (default \"\")\n",
    "password = \"\"  # Enter your Memgraph password (default \"\")\n",
    "url = \"\"  # Specify the connection URL, e.g., 'bolt://localhost:7687'\n",
    "\n",
    "graph_store = MemgraphPropertyGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Property Graph Index  \n",
    "\n",
    "Next, we build a **Property Graph Index** using the documents we previously\n",
    "loaded. This index will help structure and store our data efficiently in\n",
    "Memgraph.  \n",
    "\n",
    "- We use `OpenAIEmbedding` to generate vector embeddings for the text.  \n",
    "- We configure `SchemaLLMPathExtractor`, which utilizes an OpenAI model\n",
    "  (`gpt-4`) to extract structured knowledge from the documents.  \n",
    "- The index is stored in Memgraph using the `graph_store` connection.  \n",
    "\n",
    "By running this, we transform unstructured text into a structured property\n",
    "graph, making it easier to query and analyze relationships within the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-ada-002\"),\n",
    "    kg_extractors=[\n",
    "        SchemaLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "        )\n",
    "    ],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is created, we can explore it in the UI by visiting `http://localhost:3000/`.\n",
    "\n",
    "The easiest way to visualize the entire graph is by running a Cypher command similar to this:\n",
    "\n",
    "`MATCH p=()-[]-() RETURN p;`\n",
    "\n",
    "This command matches all of the possible paths in the graph and returns entire graph.\n",
    "\n",
    "To visualize the schema of the graph, visit the Graph schema tab and generate the new schema based on the newly created graph.\n",
    "\n",
    "To delete an entire graph, use:\n",
    "\n",
    "`MATCH (n) DETACH DELETE n;`\n",
    "\n",
    "### Querying & retrieval \n",
    "\n",
    "Now that we have structured our data into a property graph, we can retrieve\n",
    "relevant information using two different approaches:  \n",
    "\n",
    "1. **Retriever-based Search:**  \n",
    "   - We convert the index into a retriever (`as_retriever`), which allows us to\n",
    "     fetch relevant nodes related to a query.  \n",
    "   - In this example, we query, *\"What happened at Interleaf and Viaweb?\"*, and\n",
    "     print the retrieved nodes.  \n",
    "\n",
    "2. **Query Engine:**  \n",
    "   - We convert the index into a query engine (`as_query_engine`), which\n",
    "     provides a more detailed response by leveraging the structured graph.  \n",
    "   - The response includes a more comprehensive answer based on the extracted\n",
    "     relationships.  \n",
    "\n",
    "This step allows us to interact with our graph and extract meaningful insights\n",
    "from the indexed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(include_text=False)\n",
    "\n",
    "# Example query: \"What happened at Interleaf and Viaweb?\"\n",
    "nodes = retriever.retrieve(\"What happened at Interleaf and Viaweb?\")\n",
    "\n",
    "# Output results\n",
    "print(\"Query Results:\")\n",
    "for node in nodes:\n",
    "    print(node.text)\n",
    "\n",
    "# Alternatively, using a query engine\n",
    "query_engine = index.as_query_engine(include_text=True)\n",
    "\n",
    "# Perform a query and print the detailed response\n",
    "response = query_engine.query(\"What happened at Interleaf and Viaweb?\")\n",
    "print(\"\\nDetailed Query Response:\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "289d8ae9ac585fcc15d0d9333c941ae27bdf80d3e799883224b20975f2046730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
