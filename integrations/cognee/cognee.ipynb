{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognee x Memgraph integration\n",
    "\n",
    "This notebook demonstrates how to integrate\n",
    "[Cognee](https://github.com/cognee-ai/cognee) with\n",
    "[Memgraph](https://memgraph.com), a graph database platform, to automatically\n",
    "convert unstructured text into a semantically searchable knowledge graph using\n",
    "Large Language Models (LLMs). \n",
    "\n",
    "Cognee is an AI-powered toolkit for cognitive search and graph-based knowledge\n",
    "representation. It uses LLMs to break down natural language into structured\n",
    "concepts and relationships, storing them as graphs in Memgraph for further\n",
    "querying and visualization.\n",
    "\n",
    "This notebook demonstrates how to convert Hacker News threadsinto a live\n",
    "semantic knowledge graph using LLM-powered processing via Cognee and real-time\n",
    "graph storage via Memgraph.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To follow along, you'll need:\n",
    "\n",
    "1. **Docker**: Ensure [Docker](https://www.docker.com/) is installed and running\n",
    "   in the background. \n",
    "\n",
    "2. **Memgraph**: The easiest way to run Memgraph is using the following\n",
    "   commands:\n",
    "\n",
    "For Linux/macOS: `curl https://install.memgraph.com | sh`\n",
    "\n",
    "For Windows: `iwr https://windows.memgraph.com | iex`\n",
    "\n",
    "This will launch Memgraph at `localhost:3000`.\n",
    "\n",
    "3. **Python 3.10+**: For our pipeline\n",
    "\n",
    "4. **OpenAI API Key**: For LLM processing\n",
    "\n",
    "5. **Neccessary dependencies**: To install, open your terminal and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cognee dlt requests python-dateutil neo4j python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "We'll load the environment variables used to configure the LLM and graph\n",
    "database providers. These will be pulled from a `.env` file (which you must\n",
    "create securely — don’t share API keys!). In this example, we're using OpenAI.\n",
    "\n",
    "Create a file named `.env` in your project root with the following content:\n",
    "\n",
    "```\n",
    "# LLM Configuration\n",
    "LLM_API_KEY=sk-your-openai-api-key\n",
    "LLM_MODEL=openai/gpt-4o-mini\n",
    "LLM_PROVIDER=openai\n",
    "EMBEDDING_PROVIDER=openai\n",
    "EMBEDDING_MODEL=openai/text-embedding-3-large\n",
    "\n",
    "# Memgraph Configuration\n",
    "GRAPH_DATABASE_PROVIDER=memgraph\n",
    "GRAPH_DATABASE_URL=bolt://localhost:7687\n",
    "GRAPH_DATABASE_USERNAME=\"\"\n",
    "GRAPH_DATABASE_PASSWORD=\"\"\n",
    "\n",
    "# Hacker News API\n",
    "HN_API_BASE=https://hacker-news.firebaseio.com/v0\n",
    "```\n",
    "\n",
    "## Building the pipeline\n",
    "\n",
    "Let's first load our environment in notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction from HackerNews\n",
    "\n",
    "Our pipeline starts by extracting data from the Hacker News API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "from typing import Iterator, Dict, Any\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "HN_API_BASE = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "@dlt.resource(table_name=\"posts\", write_disposition=\"merge\", primary_key=\"id\")\n",
    "def get_posts_incremental(\n",
    "    updated_at=dlt.sources.incremental(\"time\", initial_value=0)\n",
    ") -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Extract posts from Hacker News API with incremental loading\"\"\"\n",
    "\n",
    "    # Get latest stories\n",
    "    top_stories_response = requests.get(f\"{HN_API_BASE}/topstories.json\")\n",
    "    top_story_ids = top_stories_response.json()\n",
    "\n",
    "    new_stories_response = requests.get(f\"{HN_API_BASE}/newstories.json\")\n",
    "    new_story_ids = new_stories_response.json()\n",
    "\n",
    "    all_story_ids = list(set(top_story_ids + new_story_ids))[:20]\n",
    "\n",
    "    print(f\"Total story IDs to check: {len(all_story_ids)}\")\n",
    "\n",
    "    for story_id in all_story_ids:\n",
    "        try:\n",
    "            item_response = requests.get(f\"{HN_API_BASE}/item/{story_id}.json\")\n",
    "            if item_response.status_code == 200:\n",
    "                item = item_response.json()\n",
    "\n",
    "                if item and item.get('type') == 'story':\n",
    "                    item_time = item.get('time', 0)\n",
    "                    if item_time > updated_at.last_value:\n",
    "                        # Prepare data for Cognee processing\n",
    "                        item['created_at'] = datetime.fromtimestamp(item['time'])\n",
    "                        item['extracted_at'] = datetime.now()\n",
    "                        item['content_for_cognee'] = f\"Title: {item.get('title', '')}. {item.get('text', '')}\"\n",
    "                        \n",
    "                        print(f\"Yielding post ID {item['id']} titled: {item.get('title', '')}\")\n",
    "\n",
    "                        yield item\n",
    "                    else:\n",
    "                        print(f\"Skipping post {item.get('id')} (old)\")\n",
    "\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching story {story_id}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cognee integration for knowledge graph generation\n",
    "\n",
    "Now, you may integrate Cognee to process the extracted text and build your\n",
    "knowledge graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cognee\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class CogneeMemgraphProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    async def process_posts(self, posts_data):\n",
    "        \"\"\"Process posts through Cognee to build knowledge graph\"\"\"\n",
    "\n",
    "        for post in posts_data:\n",
    "            try:\n",
    "                # Add post content to Cognee\n",
    "                content = post.get('content_for_cognee', '')\n",
    "                if content:\n",
    "                    print(f\"Adding post to Cognee: {post.get('title', '')}\")\n",
    "                    await cognee.add(content, dataset_name=\"hackernews_posts\")\n",
    "\n",
    "                # Add metadata as structured data\n",
    "                metadata = {\n",
    "                    \"post_id\": post.get('id'),\n",
    "                    \"author\": post.get('by'),\n",
    "                    \"score\": post.get('score', 0),\n",
    "                    \"url\": post.get('url'),\n",
    "                    \"created_at\": post.get('created_at')\n",
    "                }\n",
    "                metadata_str = \". \".join(f\"{k}: {v}\" for k, v in metadata.items())\n",
    "                await cognee.add(metadata, dataset_name=\"hackernews_metadata\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {post.get('id')}: {e}\")\n",
    "\n",
    "        # Build the knowledge graph\n",
    "        print(\"Building knowledge graph with Cognee...\")\n",
    "        await cognee.cognify()\n",
    "        print(\"Knowledge graph construction completed!\")\n",
    "\n",
    "    async def search_knowledge_graph(self, query: str):\n",
    "        \"\"\"Perform semantic search on the knowledge graph\"\"\"\n",
    "        results = await cognee.search(query_text=query)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize your data in Memgraph\n",
    "\n",
    "Now that the graph is created, we can explore it in the UI by visiting\n",
    "`http://localhost:3000/`.\n",
    "\n",
    "### Explore the graph\n",
    "\n",
    "Use Cypher queries to explore your knowledge graph:\n",
    "\n",
    "```\n",
    "-- View the entire graph structure\n",
    "MATCH p=()-[]-() RETURN p LIMIT 100;\n",
    "\n",
    "-- Find all entities related to \"AI\" or \"artificial intelligence\"\n",
    "MATCH (n)\n",
    "WHERE n.name CONTAINS \"AI\" OR n.name CONTAINS \"artificial intelligence\"\n",
    "RETURN n;\n",
    "\n",
    "-- Discover relationships between programming languages\n",
    "MATCH (lang1)-[r]-(lang2)\n",
    "WHERE lang1.type = \"programming_language\" AND lang2.type = \"programming_language\"\n",
    "RETURN lang1, r, lang2;\n",
    "\n",
    "-- Find the most connected entities (high centrality)\n",
    "MATCH (n)-[r]-()\n",
    "RETURN n.name, count(r) as connections\n",
    "ORDER BY connections DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This integration demonstrates how fast-moving online discussions, like those on Hacker News, can be transformed into queryable knowledge graphs using Cognee and Memgraph.\n",
    "\n",
    "By combining the Hacker News API with AI-based semantic understanding and high-performance graph database technology, you’ve created a system that can:\n",
    "\n",
    "- **Automatically understand** the semantic content of discussions\n",
    "- **Discover hidden relationships** between concepts and entities\n",
    "- **Enable smart search** that goes beyond keyword matching\n",
    "- **Provide visual insights** through graph exploration\n",
    "- **Scale to handle** large volumes of real-time data\n",
    "\n",
    "The future of knowledge management lies in systems that can think, reason, and discover insights the way humans. With this integration, you’re one step closer to that reality!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.16 ('cognee_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e20c0ce903d62983291697e6c8a0c04c6dc5f192d75a0a9d78908ad8de17c071"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
