{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Agentic GraphRAG powered by Memgraph 3.0 \n",
    "\n",
    "This demo highlights Agentic GraphRAG, a system that harnesses Memgraph 3.0 and its built-in tools—including vector search, BFS, PageRank, and schema management—to power AI-driven graph applications.\n",
    "\n",
    "The demo classifies user queries, generates Cypher statements, and executes them on Memgraph. Question classification, tool selection, and query parameterization are dynamically handled by an agent that interacts with the OpenAI API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites  \n",
    "\n",
    "In order to try this demo, you first need to start Memgraph. You should start Memgraph with the schema info enabled. Here is the command you can use to start Memgraph: \n",
    "\n",
    "```\n",
    "docker run -d --name memgraph_graphRAG -p 7687:7687 -p 7444:7444 memgraph/memgraph-mage:3.0-memgraph-3.0 --log-level=TRACE --also-log-to-stderr --schema-info-enabled=True \n",
    "```\n",
    "\n",
    "You should also install the dependencies needed for this demo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit==1.41.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (1.41.1)\n",
      "Requirement already satisfied: neo4j==5.26.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (5.26.0)\n",
      "Requirement already satisfied: sentence-transformers==3.3.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (3.3.1)\n",
      "Requirement already satisfied: openai==1.58.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 4)) (1.58.1)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: tiktoken==0.8.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (0.8.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: packaging<25,>=20 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from streamlit==1.41.1->-r ../requirements.txt (line 1)) (6.4.2)\n",
      "Requirement already satisfied: pytz in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from neo4j==5.26.0->-r ../requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (4.48.1)\n",
      "Requirement already satisfied: tqdm in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (0.28.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from openai==1.58.1->-r ../requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from openai==1.58.1->-r ../requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from openai==1.58.1->-r ../requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from openai==1.58.1->-r ../requirements.txt (line 4)) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from openai==1.58.1->-r ../requirements.txt (line 4)) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from openai==1.58.1->-r ../requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from tiktoken==0.8.0->-r ../requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: jinja2 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (1.24.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai==1.58.1->-r ../requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.41.1->-r ../requirements.txt (line 1)) (4.0.12)\n",
      "Requirement already satisfied: certifi in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai==1.58.1->-r ../requirements.txt (line 4)) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai==1.58.1->-r ../requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.58.1->-r ../requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai==1.58.1->-r ../requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai==1.58.1->-r ../requirements.txt (line 4)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit==1.41.1->-r ../requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit==1.41.1->-r ../requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from rich<14,>=10.14.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from rich<14,>=10.14.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: networkx in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: setuptools in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers==3.3.1->-r ../requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.41.1->-r ../requirements.txt (line 1)) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from jinja2->altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (0.22.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/antejavor/repos/ai-demos/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit==1.41.1->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dependencies have been installed, the next step is to define a `.env` file and pass in the `OPENAI_API_KEY` that will hold the key for OpenAPI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GraphRAG demo\n",
    "\n",
    "First, we import the necessary libraries and modules. We use the OpenAI API for LLM, Sentence Transformers for vector embeddings, and the Neo4j client for connecting to Memgraph. The rest of the libraries are utilities for different smaller subtasks that will be discussed a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import neo4j\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Any\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Defining the model \n",
    "\n",
    "The next step is to define the model we want to use for the LLM API. Feel free to change this to any model you prefer. In this case, we are using GPT-4o-2024-08-06. We also need to initialize logging to track the progress of the agentic GraphRAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Predefined model used.\n",
    "MODEL = {\n",
    "    \"name\": \"gpt-4o-2024-08-06\",\n",
    "    \"context_window\": 128000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tool response \n",
    "\n",
    "First we define a class `ToolResponse` to represent the response from the tools in the pipeline. This class contains a `status` flag and the `results` of the tool execution.\n",
    "If the status is `True`, the tool execution was successful, and the `results` contain the output. If the status is `False`, the tool execution failed, and results are not valid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response format\n",
    "class ToolResponse():\n",
    "    def __init__(self, status=False, results=\"\"):\n",
    "        self.status = status\n",
    "        self.results = results\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Status: {self.status}, Results: {self.results}\"\n",
    "\n",
    "    def set_status(self, status: bool):\n",
    "        self.status = status\n",
    "        return self  \n",
    "\n",
    "    def set_results(self, results: str):\n",
    "        self.results = results\n",
    "        return self  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured outputs\n",
    "\n",
    "LLMs can provide bad responses, hallucinations or unnecessary context in the answer. Structured outputs help you get predictable responses from an LLM. To get a structured output from the LLM calls, we need to define the output schema. Each LLM can have a different output schema, depending on what the agent is set to do. \n",
    "Read more about structured outputs on the [OpenAI docs](https://platform.openai.com/docs/guides/structured-outputs). \n",
    "\n",
    "Each of the classes below defines the Pydantic model of a response that is expected from the LLM. If we provide the appropriate prompt and model, the LLM should make an output in the type and format we want. \n",
    "\n",
    "For example `QuestionType` defines the `type` and `explanation` in the string format, LLM will extract the values based on the prompt description, question and the pydantic model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Agent generation of a Cypher question\n",
    "class CypherQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "# Agent response for tool selection\n",
    "class ToolSelection(BaseModel):\n",
    "    first_pick: str\n",
    "    second_pick: str\n",
    "\n",
    "# Agent generation for number of similar nodes and number of hops\n",
    "class StructureQuestionData(BaseModel):\n",
    "    number_of_similar_nodes: int\n",
    "    number_of_hops: int\n",
    "\n",
    "# Agent generation for number of nodes in the PageRank\n",
    "class PageRankNodes(BaseModel):\n",
    "    number_of_nodes: int\n",
    "\n",
    "\n",
    "# Agent response to the user question\n",
    "class QuestionType(BaseModel):\n",
    "    type: str\n",
    "    explanation: str\n",
    "\n",
    "# Community summary generation\n",
    "class Community(BaseModel):\n",
    "    summary: str    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question classification \n",
    "\n",
    "One of the responses the LLMs need to give in the structured format is the `QuestionType`. The question type defines what **set of tools** can be applied to answer a specific question. \n",
    "\n",
    "Question types are defined as **Retrieval**, **Structure**, **Global** and **Database**. \n",
    "\n",
    "The function `classify_the_question` takes the user's question and classifies it into one of these types. \n",
    "\n",
    "The example below highlights the decision making of LLMs based on the question and the output in the structured format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_the_question(openai_client, user_question: str) -> Dict:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Classify the following user question into query type\n",
    "\n",
    "    Query Types:\n",
    "    - Retrieval\n",
    "    - Structure \n",
    "    - Global\n",
    "    - Database\n",
    "\n",
    "    Each type of question has different characteristics.\n",
    "    - Retrieval: Direct Lookups, specific and well-defined. The query seeks information about specific entities (nodes or relationships). \n",
    "    - Structure: Exploratory, the query seeks information about the structure of the graph, close relationships between entities, or properties of nodes.\n",
    "    - Global: The query seeks context about the entire graph, community, such as the most important node or global trends in graph. \n",
    "    - Database: The query seeks statistical information about the database, such as index information, node count, or relationship count, config etc.\n",
    "\n",
    "    Example of a questions for each type:\n",
    "    - Retrieval: How old is a person with the name \"John\"? \n",
    "    - Structure: Does John have a job? Is John a friend of Mary? Are there any people who are friends with John?\n",
    "    - Globals: What is the most important node in the graph? \n",
    "    - Database: What indexes does Memgraph have?\n",
    "\n",
    "    In the explanation, provide a brief description of the type of question, and why you classified it as such. \n",
    "\n",
    "    The question is in <Question> </Question> format.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    user_question = f\"<Question>{user_question}</Question>\"\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_question},\n",
    "        ],\n",
    "        response_format=QuestionType,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool selection pipe \n",
    "\n",
    "Once the question type is selected, we can apply a specific tool to that particular question type. Different questions will be presented with different tools for answering the question. Ideally, this prompt would be fully dynamic in a sense where all Memgraph tools will be available to the agent context, so he can pick freely a tool he sees fit for any question. In this case, we have a hardcoded list of possible options.\n",
    "\n",
    "If you take a look at the `response_format` and the `prompt_developer` you will notice that LLM will pick the two options for solving the problem, the first tool and the second tool as a backup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tool_selection_pipe(openai_client, user_question, question_type) -> Dict:\n",
    "    question = f\"<Question>{user_question}</Question>\"\n",
    "    question_type = f\"<Type>{question_type}</Type>\"\n",
    "    prompt_developer = f\"\"\"\n",
    "\n",
    "    Based on the question type and the user's question, select the most appropriate tool option and second option as a backup to answer the question:\n",
    "\n",
    "    Retrieval - direct lookups, specific and well-defined. The query seeks information about specific entities (nodes or relationships).\n",
    "    Options: \n",
    "        - Cypher: A tool that generates a Cypher query based on the user's question and the database schema.\n",
    "        - Vector Relevance Expansion: A tool that finds the most similar nodes based on the user's question and the database schema.\n",
    "    Structure - exploratory, the query seeks information about the structure of the graph, close relationships between entities, or properties of nodes.\n",
    "    Options:\n",
    "        - Cypher: A tool that generates a Cypher query based on the user's question and the database schema.\n",
    "        - Vector Relevance Expansion: A tool that finds the most similar nodes based on the user's question and the database schema.\n",
    "    Global - the query seeks context about the entire graph, community, such as the most important node or global trends in graph.\n",
    "    Options:\n",
    "        - PageRank: A tool that provides PageRank information about the graph and its nodes, it can help with identifying the most important nodes.\n",
    "        - Community: A tool that provides communities information about the graph, it contains the summary of the community, and can help with global insights.\n",
    "    Database - the query seeks statistical information about the database, such as index information, node count, or relationship count, config etc.\n",
    "    Options:\n",
    "        - Schema: A tool that provides schema information about the dataset and datatypes.\n",
    "        - Config: A tool that provides configuration information about the database.\n",
    "        - Cypher: A tool that generates a Cypher query based on the user's question and the database schema.\n",
    "\n",
    "    The question is in <Question> </Question> format, and the type of the question is <Type> </Type>.\n",
    "\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": prompt_developer},\n",
    "        {\"role\": \"user\", \"content\": question + question_type},\n",
    "\n",
    "    ]\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=messages,\n",
    "        response_format=ToolSelection,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools in the GraphRAG demo\n",
    "\n",
    "### Schema info\n",
    "\n",
    "The schema info tool is responsible for getting the graph schema information from the database. Providing a brief summary of the graph schema can help the user understand the structure of the graph and formulate more specific queries. That includes the node, relationship and property types.\n",
    "\n",
    "The function `get_schema_string` retrieves the fresh schema information from the database and formats it to a human-readable string.  This function [utilizes Memgraph's](https://memgraph.com/docs/querying/schema) `SHOW SCHEMA INFO` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_schema_string(db_client) -> str:\n",
    "    \n",
    "    with db_client.session() as session:\n",
    "        schema = session.run(\"SHOW SCHEMA INFO\")\n",
    "        schema_info = json.loads(schema.single().value())\n",
    "        nodes = schema_info[\"nodes\"]\n",
    "        edges = schema_info[\"edges\"]\n",
    "        node_indexes = schema_info[\"node_indexes\"]\n",
    "        edge_indexes = schema_info[\"edge_indexes\"]\n",
    "\n",
    "        schema_str = \"Nodes:\\n\"\n",
    "        for node in nodes:\n",
    "            properties = \", \".join(\n",
    "                f\"{prop['key']}: {', '.join(t['type'] for t in prop['types'])}\"\n",
    "                for prop in node[\"properties\"]\n",
    "            )\n",
    "            schema_str += f\"Labels: {node['labels']} | Properties: {properties}\\n\"\n",
    "\n",
    "        schema_str += \"\\nEdges:\\n\"\n",
    "        for edge in edges:\n",
    "            properties = \", \".join(\n",
    "                f\"{prop['key']}: {', '.join(t['type'] for t in prop['types'])}\"\n",
    "                for prop in edge[\"properties\"]\n",
    "            )\n",
    "            schema_str += f\"Type: {edge['type']} | Start Node Labels: {edge['start_node_labels']} | End Node Labels: {edge['end_node_labels']} | Properties: {properties}\\n\"\n",
    "\n",
    "        schema_str += \"\\nNode Indexes:\\n\"\n",
    "        for index in node_indexes:\n",
    "            schema_str += (\n",
    "                f\"Labels: {index['labels']} | Properties: {index['properties']}\\n\"\n",
    "            )\n",
    "\n",
    "        schema_str += \"\\nEdge Indexes:\\n\"\n",
    "        for index in edge_indexes:\n",
    "            schema_str += f\"Type: {index['type']} | Properties: {index['properties']}\\n\"\n",
    "\n",
    "        return schema_str\n",
    "\n",
    "def schema_tool(db_client) -> ToolResponse:\n",
    "    return ToolResponse(True, get_schema_string(db_client))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Cypher \n",
    "\n",
    "The function `text_to_Cypher` translates a natural language question into a Cypher query using the LLM generation.\n",
    "It leverages the schema info to generate accurate queries and includes error correction and retry logic.\n",
    "\n",
    "If the query is failing to execute, the tool will try to self-recover based on the database error message or expanded prompt. \n",
    "\n",
    "\n",
    "The `generate_cypher_query` function will call LLM to provide a cypher query in structured format based on the given prompt messages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_Cypher(db_client, openai_client, user_question) -> Dict:\n",
    "    logger.info(\"Running text_to_cypher tool\")\n",
    "\n",
    "    schema = get_schema_string(db_client)\n",
    "    prompt_user = f\"\"\"\n",
    "\n",
    "    User Question: \"{user_question}\"\n",
    "    Schema: {schema}\n",
    "\n",
    "    Based on schema and question, generate a Cypher query that directly corresponds to the user's intent.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_developer = f\"\"\"\n",
    "    Your task is to directly translate natural language\n",
    "    inquiry into precise and executable Cypher query for Memgraph database.\n",
    "    You will utilize a provided database schema to understand the structure,\n",
    "    nodes and relationships within the Memgraph database.\n",
    "\n",
    "    Rules:\n",
    "    - Use provided node and relationship labels and property names from the\n",
    "    schema which describes the database's structure. Upon receiving a user question, synthesize the\n",
    "    schema to craft a precise Cypher query that directly corresponds to\n",
    "    the user's intent.\n",
    "    - Generate valid executable Cypher queries on top of Memgraph database.\n",
    "    - Use Memgraph MAGE procedures instead of Neo4j APOC procedures.\n",
    "\n",
    "    With all the above information and instructions, generate Cypher query\n",
    "    for the user question.\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count_user = len(encoding.encode(prompt_user))\n",
    "    token_count_developer = len(encoding.encode(prompt_developer))\n",
    "    token_count = token_count_user + token_count_developer\n",
    "    logger.info(f\"Token count on prompt : {token_count}\")\n",
    "\n",
    "    prompt_chain = [\n",
    "            {\"role\": \"developer\", \"content\": prompt_developer},\n",
    "            {\"role\": \"user\", \"content\": prompt_user},\n",
    "    ]\n",
    "\n",
    "    tool_response = ToolResponse()\n",
    "\n",
    "    query = \"\"\n",
    "    if token_count <= MODEL[\"context_window\"]:\n",
    "        query = generate_cypher_query(openai_client, prompt_chain)\n",
    "    else:\n",
    "        return tool_response.set_status(False).set_results(\"Token count exceeded the limit.\")\n",
    "\n",
    "    logger.info(\"### Cypher Query:\")\n",
    "    logger.info(query)\n",
    "    \n",
    "    res = []\n",
    "    with db_client.session() as session:\n",
    "        for _ in range(3):  # Try correction process up to 3 times\n",
    "            try:\n",
    "                results = session.run(query)\n",
    "                if not results.peek():\n",
    "                    raise ValueError(\n",
    "                        \"The query did not return any results. There is a possible issue with the query \"\n",
    "                        \"labels and parameters, if you are matching strings consider matching them in the case-insensitive way.\"\n",
    "                    )\n",
    "                for record in results:\n",
    "                    res.append(record)\n",
    "\n",
    "                return tool_response.set_status(True).set_results(res)\n",
    "\n",
    "            except (ValueError, Exception) as e:\n",
    "                error_type = \"ValueError\" if isinstance(e, ValueError) else \"Error\"\n",
    "                logger.error(f\"{error_type} in running the query:\")\n",
    "                logger.error(e)\n",
    "                error_message = str(e)\n",
    "\n",
    "                prompt_correction = f\"\"\"\n",
    "                The following Cypher query generated a {error_type}:\n",
    "                Query: {query}\n",
    "                Error: {error_message}\n",
    "                Question: {user_question}\n",
    "\n",
    "                Please correct the Cypher query based on the error, schema and question.\n",
    "                \"\"\"\n",
    "                prompt_chain.append({\"role\": \"assistant\", \"content\": query})\n",
    "                prompt_chain.append({\"role\": \"developer\", \"content\": prompt_correction})\n",
    "\n",
    "                query = generate_cypher_query(openai_client, prompt_chain)\n",
    "                logger.info(\"### Corrected Cypher Query:\")\n",
    "                logger.info(query)\n",
    "\n",
    "        return tool_response.set_status(False).set_results(\"Error in running the query.\")\n",
    "\n",
    "def generate_cypher_query(openai_client, prompt_messages):\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=prompt_messages,\n",
    "        response_format=CypherQuery,\n",
    "    )\n",
    "    return completion.choices[0].message.parsed.query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Config Tool\n",
    "\n",
    "Config tool used to retrieve the configuration information form the database. If the user has some question about Memgraph configuration this tool could help an LLM answer that question. \n",
    "\n",
    "In this case the `SHOW CONFIG` [query](https://memgraph.com/docs/database-management/configuration) is being used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def config_tool(db_client) -> ToolResponse:\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            config = session.run(\"SHOW CONFIG\")\n",
    "            config_str = \"Configurations:\\n\"\n",
    "            for record in config:\n",
    "                config_str += f\"Name: {record['name']} | Default Value: {record['default_value']} | Current Value: {record['current_value']} | Description: {record['description']}\\n\"\n",
    "            return ToolResponse(True, config_str)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the Config tool query.\")\n",
    "        return ToolResponse(False, \"Error in running the Config tool query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank \n",
    "\n",
    "PageRank tool will return the list of running a [PageRank](https://memgraph.com/docs/advanced-algorithms/available-algorithms/pagerank) algorithm on top of the graph data in Memgraph. When you think about any tool, each tool can be specifically modified to the particular question ask. \n",
    "\n",
    "Here is the example questions: \n",
    "\n",
    "\"Is the Coca Cola the most important company in the dataset?\"  - For this question you are probably curious what is the most important node in the dataset, and it is just one. \n",
    "\"Is the Coca Cola in the 100 most important companies in the dataset?\" - For this question you are interested in the 100 most important nodes in the dataset (companies). \n",
    "\n",
    "Now both questions utilize PageRank to get the response, but dynamically modeling and adapting the tool based on the LLM choice will call PageRank with the proper configuration. \n",
    "\n",
    "The LLM call `page_rank_choice` is trying to guess the number of nodes to get from page rank tool, and pass that information to the actual `page_rank_tool`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def page_rank_choice(openai_client, user_question) -> Dict:\n",
    "    question = f\"<Question>{user_question}</Question>\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the provided question, try to guess how many nodes should be returned from the PageRank in the assesment. \n",
    "    The question is in <Question> </Question> format.\n",
    "    \"\"\"\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        response_format=PageRankNodes,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "def page_rank_tool(db_client, openai_client, user_question) -> ToolResponse:\n",
    "\n",
    "    prompt_developer = f\"\"\"\n",
    "    Based on the provided question, try to guess how many nodes should be returned from the PageRank in the assesment. \n",
    "    The question is in <Question> </Question> format.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": prompt_developer},\n",
    "        {\"role\": \"user\", \"content\": \"<Question>\" + user_question + \"</Question>\"},\n",
    "    ]\n",
    "\n",
    "    choice = page_rank_choice(openai_client, user_question)\n",
    "\n",
    "    logger.info(\"Running the PageRank tool\")\n",
    "    logger.info(f\"Number of nodes: {choice.number_of_nodes}\")\n",
    "\n",
    "    with db_client.session() as session:\n",
    "        try:\n",
    "            result = session.run(f\"CALL pagerank.get() YIELD node, rank RETURN node, rank LIMIT {choice.number_of_nodes};\")\n",
    "            result_str = \"\"\n",
    "            for record in result:\n",
    "                node = record[\"node\"]\n",
    "                properties = {k: v for k, v in node.items() if k != \"embedding\"}\n",
    "                result_str += f\"Node: {properties}, Rank: {record['rank']}\\n\"\n",
    "            \n",
    "            logger.info(\"Page rank successful\") \n",
    "            logger.info(result_str)\n",
    "            return ToolResponse(True, result_str)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in running the PageRank tool query.\")\n",
    "            return ToolResponse(False, \"Error in running the PageRank tool query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community summarization tool \n",
    "\n",
    "If the question about dataset is addressing some global trend or a bigger set of nodes and edges, community algorithms can be used to detect communities. These communities can then be further analyzed to see how they fit into the bigger picture in the graph. \n",
    "\n",
    "In this particular example we have used a [Louvain community detection](https://memgraph.com/docs/advanced-algorithms/available-algorithms/community_detection) to detect communities. They are being pre-computed on the start of the application in the `precompute_community_summary` function.  \n",
    "\n",
    "The function calculates the community, after that it takes a community sub-graph, and passes the info to LLM, that computes the community summary in 5 sentences. After that the community node is created for each community that holds the summary. \n",
    "\n",
    "This approach is just a demonstration of how question on global trends can be handled, Leiden with the hierarchical modeling is a better choice since the communities can be explored dynamically. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_community_summary(db_client, openai_client) -> Dict:\n",
    "\n",
    "    number_of_communities = 0\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            CALL community_detection.get()\n",
    "            YIELD node, community_id \n",
    "            SET node.community_id = community_id;\n",
    "            \"\"\"\n",
    "            )\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            RETURN count(distinct n.community_id) as community_count;\n",
    "            \"\"\"\n",
    "            )\n",
    "            for record in result:\n",
    "                number_of_communities = record['community_count']\n",
    "                print(f\"Number of communities: {record['community_count']}\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the community detection query.\")\n",
    "        return False; \n",
    "    \n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            communities = []\n",
    "            for i in range(0, number_of_communities):\n",
    "                community_string = \"\"\n",
    "                community_id = 0\n",
    "                result = session.run(f\"\"\"\n",
    "                MATCH (start), (end) \n",
    "                WHERE start.community_id = {i} AND end.community_id = {i} AND id(start) < id(end)\n",
    "                MATCH p = (start)-[*..1]-(end)\n",
    "                RETURN p; \n",
    "                \"\"\")\n",
    "                for record in result:\n",
    "                    path = record['p']\n",
    "                    for rel in path.relationships:\n",
    "                        start_node = rel.start_node\n",
    "                        end_node = rel.end_node\n",
    "                        start_node_properties = {k: v for k, v in start_node.items() if k != 'embedding'}\n",
    "                        end_node_properties = {k: v for k, v in end_node.items() if k != 'embedding'}\n",
    "                        community_string += f\"({start_node_properties})-[:{rel.type}]->({end_node_properties})\\n\"\n",
    "                        community_id = i\n",
    "                communities.append({\"id\": community_id, \"data\": community_string})\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the community detection query.\")\n",
    "        return False;\n",
    "        \n",
    "    logger.info(\"Total number of communities:\")\n",
    "    logger.info(number_of_communities)\n",
    "    community_summary = []\n",
    "    for community in communities:\n",
    "        community_id = community['id']\n",
    "        community_string = community['data']\n",
    "        try:\n",
    "            logging.info(f\"Generating summary for community {community_id}\")\n",
    "            prompt = community_prompt(openai_client, community_string)\n",
    "            community_summary.append({\"id\": community_id, \"summary\": prompt.summary})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in generating summary for community {community_id} and community string {community_string}\")\n",
    "            logger.error(e)\n",
    "            return False;\n",
    "\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            for community in community_summary:\n",
    "                community_id = community['id']\n",
    "                summary = community['summary']\n",
    "                session.run(\n",
    "                    \"CREATE (c:Community { id: $id, summary: $summary})\",\n",
    "                    summary=summary, \n",
    "                    id=community_id\n",
    "                )\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the community detection query.\")\n",
    "        return False;\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def community_prompt(openai_client, community_string) -> Dict:\n",
    "    prompt = f\"Summarize the following community information into 5 to 10 sentences, you will get the community string in the <Community> </Community> format\"\n",
    "    prompt_community= f\"<Community>{community_string}</Community>\"\n",
    "\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_community},\n",
    "        ],\n",
    "        response_format=Community,\n",
    "    )\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "def check_if_community_summary_exists(db_client) -> bool:\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (c:Community)\n",
    "            RETURN count(c) as community_count;\n",
    "            \"\"\"\n",
    "            )\n",
    "            for record in result:\n",
    "                if record['community_count'] > 0:\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the community detection query.\")\n",
    "        logger.error(e) \n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After communities are pre-computed, the community tool calls a simple query that matches all communities, returns them and provides an insight into what is happening in each community. If the dataset scale is large and there are a lot of communities, vector search can be applied on top of the community summary to fetch only the communities that are most similar to the actual question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_tool(db_client) -> ToolResponse:\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            result = session.run(\"MATCH (n:Community) RETURN n.id, n.summary;\")\n",
    "            result_str = \"\"\n",
    "            for record in result:\n",
    "                result_str += f\"Community ID: {record['n.id']}, Summary: {record['n.summary']}\\n\"\n",
    "            return ToolResponse(True, result_str)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the Community tool query.\")\n",
    "        return ToolResponse(False, \"Error in running the Community tool query.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector search and relevance expansion \n",
    "\n",
    "If the question is about the graph structure, finding a pivot point (node) in the graph and expanding from that node will give the answer about the graph structure that encodes the knowledge. An example would be, \"How is Coca cola related to rest of the dataset?\". \n",
    "\n",
    "Question is quite open but it points into direction of understanding the relationships around Coca-Cola. In order to find the pivot node, we create embeddings on top of node properties, this is computed in the `compute_node_embeddings`, each node also gets the `Entity` label since we are performing vector search on all the nodes in the database. Details of the index are visible in the `index_setup`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def index_setup(db_client):\n",
    "    with db_client.session() as session:\n",
    "        print(\"Creating the vector index...\")\n",
    "        session.run(\n",
    "            \"\"\"\n",
    "            CREATE VECTOR INDEX index_name ON :Entity(embedding) WITH CONFIG {\"dimension\": 384, \"capacity\": 2000, \"metric\": \"cos\",\"resize_coefficient\": 2};\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "def compute_node_embeddings(db_client):\n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    with db_client.session() as session:\n",
    "        # Retrieve all nodes\n",
    "        result = session.run(\"MATCH (n) RETURN n\")\n",
    "        for record in result:\n",
    "            node = record[\"n\"]\n",
    "            # Check if the node already has an embedding\n",
    "            if \"embedding\" in node:\n",
    "                print(\"Embedding already exists\")\n",
    "                return\n",
    "\n",
    "            # Combine node labels and properties into a single string\n",
    "            node_data = (\n",
    "                \" \".join(node.labels)\n",
    "                + \" \"\n",
    "                + \" \".join(f\"{k}: {v}\" for k, v in node.items())\n",
    "            )\n",
    "            # Compute the embedding for the node\n",
    "            node_embedding = model.encode(node_data)\n",
    "\n",
    "            # Store the embedding back into the node\n",
    "            session.run(\n",
    "                f\"MATCH (n) WHERE id(n) = {node.element_id} SET n.embedding = {node_embedding.tolist()}\"\n",
    "            )\n",
    "\n",
    "        session.run(\"MATCH (n) SET n:Entity\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setup, we can now run vector search defined in `vector_relevance_expansion` to find the most similar nodes to the actual question. Then again, there is LLM making a decision in `decide_on_structure_parameters` on how many similar nodes will be considerd and how many hops will be made to find the answer. \n",
    "\n",
    "Finally, we can run the Cypher query in `get_relevant_data` to get the data that could potential answer our question about data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_relevance_expansion(db_client, openai_client, user_question) -> Dict:\n",
    "\n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    question_embedding = model.encode(user_question)\n",
    "\n",
    "    prompt_parameters = f\"\"\"\n",
    "    You will get a question about the structure of the graph. The vector search\n",
    "    will find the most similar node based on the question embedding an node\n",
    "    embedding, and then return the data connected to the most similar nodes that\n",
    "    are hops away. Your task is to find out how many nodes should vector search\n",
    "    return and how many hops should be used to find the relevant data. If the\n",
    "    question is about undefined number of node guess the intended number of\n",
    "    nodes, by default consider 1. If the question is about undefined number of\n",
    "    hops guess the intended number of hops, by default consider 1.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [ \n",
    "        {\"role\": \"developer\", \"content\": prompt_parameters}, \n",
    "        {\"role\": \"user\", \"content\": user_question} \n",
    "        ]\n",
    "\n",
    "    structure_parameters = decide_on_structure_parameters(openai_client, messages)\n",
    "\n",
    "    logger.info(\"Structure parameters:\")\n",
    "    logger.info(structure_parameters)\n",
    "\n",
    "    nodes = find_most_similar_nodes(db_client, user_question,  question_embedding, structure_parameters.number_of_similar_nodes)\n",
    "\n",
    "\n",
    "    for node in nodes:\n",
    "        logger.info(\"Most similar nodes:\")\n",
    "        logger.info(node)\n",
    "\n",
    "    tool_response = ToolResponse()\n",
    "    if nodes is None:\n",
    "        return tool_response.set_status(False).set_results(\"No similar nodes found.\")\n",
    "\n",
    "    relevant_data = get_relevant_data(db_client, nodes, structure_parameters.number_of_hops)\n",
    "\n",
    "    return tool_response.set_status(True).set_results(relevant_data)\n",
    "\n",
    "\n",
    "\n",
    "def decide_on_structure_parameters(openai_client, messages) -> Dict:\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=messages,\n",
    "        response_format=StructureQuestionData,\n",
    "    )\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def find_most_similar_nodes(db_client, user_question,  question_embedding, number_of_similar_nodes):\n",
    "        \n",
    "    with db_client.session() as session:\n",
    "        result = session.run(\n",
    "            f\"CALL vector_search.search('index_name', {number_of_similar_nodes}, {question_embedding.tolist()}) YIELD * RETURN *;\"\n",
    "        )\n",
    "        nodes_data = []\n",
    "        for record in result:\n",
    "            node = record[\"node\"]\n",
    "            properties = {k: v for k, v in node.items() if k != \"embedding\"}\n",
    "            node_data = {\n",
    "                \"distance\": record[\"distance\"],\n",
    "                \"id\": node.element_id,\n",
    "                \"labels\": list(node.labels),\n",
    "                \"properties\": properties,\n",
    "            }\n",
    "\n",
    "            nodes_data.append(node_data)\n",
    "        print(\"All similar nodes:\")\n",
    "        for node in nodes_data:\n",
    "            print(node)\n",
    "\n",
    "        return nodes_data if nodes_data else None\n",
    "\n",
    "\n",
    "def get_relevant_data(db_client, nodes, hops):\n",
    "    paths = []\n",
    "    for node in nodes:\n",
    "        with db_client.session() as session:\n",
    "            query = (\n",
    "                f\"MATCH path=((n)-[r*..{hops}]-(m)) WHERE id(n) = {node['id']} RETURN path\"\n",
    "            )\n",
    "            result = session.run(query)\n",
    "            \n",
    "            for record in result:\n",
    "                path_data = []\n",
    "                for segment in record[\"path\"]:\n",
    "\n",
    "                    # Process start node without 'embedding' property\n",
    "                    start_node_data = {\n",
    "                        k: v for k, v in segment.start_node.items() if k != \"embedding\"\n",
    "                    }\n",
    "\n",
    "                    # Process relationship data\n",
    "                    relationship_data = {\n",
    "                        \"type\": segment.type,\n",
    "                        \"properties\": segment.get(\"properties\", {}),\n",
    "                    }\n",
    "\n",
    "                    # Process end node without 'embedding' property\n",
    "                    end_node_data = {\n",
    "                        k: v for k, v in segment.end_node.items() if k != \"embedding\"\n",
    "                    }\n",
    "\n",
    "                    # Add to path_data as a tuple (start_node, relationship, end_node)\n",
    "                    path_data.append((start_node_data, relationship_data, end_node_data))\n",
    "\n",
    "                paths.append(path_data)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool execution pipe\n",
    "\n",
    "After each of the tools have been defined here is the simple pipeline for tool execution. Each tool returns, status and results that contain data necessary to answer the question as described in the `ToolResponse` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_execution(tool: str, db_client, openai_client, user_question) -> ToolResponse:\n",
    "\n",
    "    if tool == \"Cypher\":\n",
    "        return text_to_Cypher(db_client, openai_client, user_question)\n",
    "    elif tool == \"Vector Relevance Expansion\":\n",
    "        return vector_relevance_expansion(db_client, openai_client, user_question)\n",
    "    elif tool == \"PageRank\":\n",
    "        return page_rank_tool(db_client, openai_client, user_question)\n",
    "    elif tool == \"Community\":\n",
    "        return community_tool(db_client)\n",
    "    elif tool == \"Schema\":\n",
    "        return  schema_tool(db_client)\n",
    "    elif tool == \"Config\":\n",
    "        return config_tool(db_client)\n",
    "    else:\n",
    "        return ToolResponse(False, \"Tool execution failed, tool not found.\")\n",
    "\n",
    "\n",
    "def execute_tool(tool: str, user_question: str, db_client,  openai_client ) -> ToolResponse:\n",
    "    \"\"\"\n",
    "    Executes the given tool based on its name.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    response = None\n",
    "    try:\n",
    "        logger.info(f\"Trying tool: {tool}\")\n",
    "        response = tool_execution(tool, db_client, openai_client, user_question)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing {tool}: {e}\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating final response \n",
    "\n",
    "Once the data has been passed from any tools above, the LLM tries to generate the answer based on that question. Since tool selection and tool configuration are performed dynamically and decided by the LLM, it is not possible to be certain about what will happen on every question run, but returned data should hold the information related to the question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_response(openai_client, results, user_question: str):\n",
    "    prompt = f\"\"\"\n",
    "    Using the data and the user's original question, generate a final answer:\n",
    "    User Question: \"{user_question}\"\n",
    "    Data from the database: {results}\n",
    "\n",
    "    Try to answer the user's question using just the the provided data, and the user's question.\n",
    "    \n",
    "    \"\"\"\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the demo \n",
    "\n",
    "This is the end of the code, below are the client connections, and preprocessing of data that is described earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_client():\n",
    "    return OpenAI()\n",
    "\n",
    "def get_db_client():\n",
    "    return neo4j.GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"\", \"\"))\n",
    "\n",
    "def preprocess_data(_db_client, _openai_client):\n",
    "    if not check_if_community_summary_exists(_db_client):\n",
    "        status = precompute_community_summary(db_client, openai_client)\n",
    "        if status:\n",
    "            logger.info(\"Community summary precomputed.\")\n",
    "        else:\n",
    "            logger.error(\"Error in precomputing community summary.\")\n",
    "            logger.error(\"Community questions will fail\")\n",
    "    else:\n",
    "        logger.info(\"Community summary already exists.\")\n",
    "\n",
    "    index_setup(db_client)\n",
    "    compute_node_embeddings(db_client)\n",
    "    return \"Proccessing data completed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the main function, make sure to change the `user_question` to get response on your own question: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 10:26:38,614 - INFO - Community summary already exists.\n",
      "2025-02-21 10:26:38,618 - INFO - Use pytorch device_name: mps\n",
      "2025-02-21 10:26:38,618 - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vector index...\n",
      "Embedded data: \n",
      "Embedding already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 10:26:40,681 - INFO - Community summary already exists.\n",
      "2025-02-21 10:26:40,684 - INFO - Use pytorch device_name: mps\n",
      "2025-02-21 10:26:40,684 - INFO - Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vector index...\n",
      "Embedded data: \n",
      "Embedding already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 10:27:06,584 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-21 10:27:06,606 - INFO - Question type:\n",
      "2025-02-21 10:27:06,607 - INFO - type='Retrieval' explanation=\"This question is seeking specific information about 'Coca Cola'. It is a retrieval query because it directly asks for details about a particular entity (Coca Cola), which could include attributes or direct facts related to this brand in a database or knowledge graph.\"\n",
      "2025-02-21 10:27:06,607 - INFO - Tool selection:\n",
      "2025-02-21 10:27:07,673 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-21 10:27:07,674 - INFO - Tools selected:\n",
      "2025-02-21 10:27:07,675 - INFO - first_pick='Cypher' second_pick='Vector Relevance Expansion'\n",
      "2025-02-21 10:27:07,675 - INFO - Trying tool: Cypher\n",
      "2025-02-21 10:27:07,675 - INFO - Running text_to_cypher tool\n",
      "2025-02-21 10:27:07,731 - INFO - Token count on prompt : 41629\n",
      "2025-02-21 10:27:10,122 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-21 10:27:10,129 - INFO - ### Cypher Query:\n",
      "2025-02-21 10:27:10,130 - INFO - MATCH (e:Entity:organization {detailed_type: 'Coca Cola'}) RETURN e\n",
      "2025-02-21 10:27:10,134 - ERROR - ValueError in running the query:\n",
      "2025-02-21 10:27:10,135 - ERROR - The query did not return any results. There is a possible issue with the query labels and parameters, if you are matching strings consider matching them in the case-insensitive way.\n",
      "2025-02-21 10:27:11,959 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-21 10:27:11,962 - INFO - ### Corrected Cypher Query:\n",
      "2025-02-21 10:27:11,962 - INFO - MATCH (e:Entity:organization) WHERE toLower(e.detailed_type) = 'coca cola' RETURN e\n",
      "2025-02-21 10:27:11,967 - ERROR - ValueError in running the query:\n",
      "2025-02-21 10:27:11,967 - ERROR - The query did not return any results. There is a possible issue with the query labels and parameters, if you are matching strings consider matching them in the case-insensitive way.\n",
      "2025-02-21 10:27:13,880 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-21 10:27:13,881 - INFO - ### Corrected Cypher Query:\n",
      "2025-02-21 10:27:13,881 - INFO - MATCH (e:Entity:organization) WHERE toLower(e.id) CONTAINS 'coca cola' RETURN e\n",
      "2025-02-21 10:27:13,885 - INFO - First pick: 'Cypher' succeeded.\n",
      "2025-02-21 10:27:13,885 - INFO - Generating final response\n",
      "2025-02-21 10:27:15,334 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-21 10:27:15,343 - INFO - Final response: Coca-Cola is identified as a consumer goods company. It is categorized under the main type of \"company\" and is associated with the community ID 4. This information suggests that Coca-Cola is a well-established organization in the consumer goods sector, known for producing and distributing a wide range of beverage products.\n",
      "2025-02-21 10:27:15,343 - INFO - Final response generated.\n",
      "2025-02-21 10:27:15,344 - INFO - Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def main(db_client, openai_client):\n",
    "\n",
    "    # Change it to your dataset\n",
    "    user_question = \"What can you tell me about Coca cola?\"\n",
    "\n",
    "\n",
    "    db_client = get_db_client()\n",
    "    openai_client = get_openai_client()\n",
    "\n",
    "    preprocess_data(db_client, openai_client)\n",
    "\n",
    "    \n",
    "    question_type = classify_the_question(openai_client, user_question)\n",
    "\n",
    "    logger.info(\"Question type:\")\n",
    "    logger.info(question_type)\n",
    "\n",
    "    logger.info(\"Tool selection:\")\n",
    "    tools = tool_selection_pipe(openai_client, user_question, question_type)\n",
    "\n",
    "    logger.info(\"Tools selected:\")\n",
    "    logger.info(tools)\n",
    "\n",
    "\n",
    "    response = execute_tool(tools.first_pick, user_question, db_client, openai_client)\n",
    "    if response.status:\n",
    "        logger.info(f\"First pick: '{tools.first_pick}' succeeded.\")\n",
    "    else:\n",
    "        response = execute_tool(tools.second_pick, user_question, db_client, openai_client)\n",
    "        if response.status:\n",
    "            logger.info(f\"Second pick: '{tools.second_pick}' succeeded.\")\n",
    "        else:\n",
    "            logger.error(f\"Both tools failed for question: {user_question}\")\n",
    "\n",
    "\n",
    "    if response.status is False:\n",
    "        logger.error(f\"Both tools failed for question: {user_question}\")\n",
    "    else:\n",
    "        formated_response = {\"Response\": response.results}\n",
    "\n",
    "        logger.info(\"Generating final response\")\n",
    "        final_response = generate_final_response(\n",
    "            openai_client, response.results, user_question\n",
    "        )\n",
    "        logger.info(\"Final response: \" + final_response.content)\n",
    "        logger.info(\"Final response generated.\")\n",
    "\n",
    "        logger.info(\"Pipeline completed.\")\n",
    "        \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    openai_client = get_openai_client()\n",
    "\n",
    "    db_client = get_db_client()\n",
    "\n",
    "    preprocess_data(db_client, openai_client)\n",
    "\n",
    "    main(db_client, openai_client)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
