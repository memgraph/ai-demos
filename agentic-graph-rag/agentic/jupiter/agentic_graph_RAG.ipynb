{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Agentic GraphRAG powered by Memgraph 3.0 \n",
    "\n",
    "This demo highlights Agentic GraphRAG, a system that harnesses Memgraph 3.0 and its built-in tools—including vector search, BFS, PageRank, and schema management—to power AI-driven graph applications.\n",
    "\n",
    "The demo classifies user queries, generates Cypher statements, and executes them on Memgraph. Question classification, tool selection, and query parameterization are dynamically handled by an agent that interacts with the OpenAI API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites  \n",
    "\n",
    "In order to try this demo, you first need to start Memgraph. You should start Memgraph with the schema info enabled. Here is the command you can use to start Memgraph: \n",
    "\n",
    "```\n",
    "docker run -d --name memgraph_graphRAG -p 7687:7687 -p 7444:7444 memgraph/memgraph-mage:3.0-memgraph-3.0 --log-level=TRACE --also-log-to-stderr --schema-info-enabled=True \n",
    "```\n",
    "\n",
    "You should also install the dependencies needed for this demo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dependencies have been installed, next step is to define a `.env` file and pass in the `OPENAI_API_KEY` that will hold the key for OpenAPI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Demo \n",
    "\n",
    "First, we import the necessary libraries and modules. We use the OpenAI API for LLM, Sentence Transformers for vector embeddings, and the Neo4j client for connecting to Memgraph. The rest of the libraries are utilities for different smaller subtasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import neo4j\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Defining the model \n",
    "\n",
    "The next step is to define the model we want to use for the LLM API. Feel free to change this to any model you prefer. In this case, we are using GPT-4o-2024-08-06. We also need to initialize logging to track the progress of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Predefined model used.\n",
    "MODEL = {\n",
    "    \"name\": \"gpt-4o-2024-08-06\",\n",
    "    \"context_window\": 128000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tool response \n",
    "\n",
    "# First we define a class to represent the response from the tools in the pipeline. This class contains a status flag and the results of the tool execution.\n",
    "# If the status is True, the tool execution was successful, and the results contain the output. If the status is False, the tool execution failed, and results are not valid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response format\n",
    "class ToolResponse():\n",
    "    def __init__(self, status=False, results=\"\"):\n",
    "        self.status = status\n",
    "        self.results = results\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Status: {self.status}, Results: {self.results}\"\n",
    "\n",
    "    def set_status(self, status: bool):\n",
    "        self.status = status\n",
    "        return self  \n",
    "\n",
    "    def set_results(self, results: str):\n",
    "        self.results = results\n",
    "        return self  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured outputs\n",
    "\n",
    "# In order to get a structured output from the LLMs calls, we need to define the output schema. Each LLM can have a different output schema, depending on what is agent set to do. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 5: Agent responses\n",
    "\n",
    "# Next, we define classes to represent the responses from the Agent. These classes are used to structure the data returned by the Agent calls to the OpenAI API.\n",
    "# This is based on the [structured output](https://platform.openai.com/docs/guides/structured-outputs) from the OpenAI API. \n",
    "\n",
    "# Agent generation of a Cypher question\n",
    "class CypherQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "# Agent response for tool selection\n",
    "class ToolSelection(BaseModel):\n",
    "    first_pick: str\n",
    "    second_pick: str\n",
    "\n",
    "# Agent generation for number of similar nodes and number of hops\n",
    "class StructureQuestionData(BaseModel):\n",
    "    number_of_similar_nodes: int\n",
    "    number_of_hops: int\n",
    "\n",
    "# Agent generation for number of nodes in the PageRank\n",
    "class PageRankNodes(BaseModel):\n",
    "    number_of_nodes: int\n",
    "\n",
    "\n",
    "# Agent reponse to the user question\n",
    "class QuestionType(BaseModel):\n",
    "    type: str\n",
    "    explanation: str\n",
    "\n",
    "# Community summary generation\n",
    "class Community(BaseModel):\n",
    "    summary: str    \n",
    "\n",
    "\n",
    "# Cell 6: Classify the question\n",
    "# Question types are defined here: Retrieval, Structure, Global, Database \n",
    "# The function classify_the_question takes the user question and classifies it into one of these types.\n",
    "\n",
    "def classify_the_question(openai_client, user_question: str) -> Dict:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Classify the following user question into query type\n",
    "\n",
    "    Query Types:\n",
    "    - Retrieval\n",
    "    - Structure \n",
    "    - Global\n",
    "    - Database\n",
    "\n",
    "    Each type of question has different characteristics.\n",
    "    - Retrieval: Direct Lookups, specific and well-defined. The query seeks information about specific entities (nodes or relationships). \n",
    "    - Structure: Exploratory, the query seeks information about the structure of the graph, close relationships between entities, or properties of nodes.\n",
    "    - Global: The query seeks context about the entire graph, community, such as the most important node or global trends in graph. \n",
    "    - Database: The query seeks statistical information about the database, such as index information, node count, or relationship count, config etc.\n",
    "\n",
    "    Example of a questions for each type:\n",
    "    - Retrieval: How old is a person with the name \"John\"? \n",
    "    - Structure: Does John have a job? Is John a friend of Mary? Are there any people who are friends with John?\n",
    "    - Globals: What is the most important node in the graph? \n",
    "    - Database: What indexes does Memgraph have?\n",
    "\n",
    "    In the explanation, provide a brief description of the type of question, and why you classified it as such. \n",
    "\n",
    "    The question is in <Question> </Question> format.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    user_question = f\"<Question>{user_question}</Question>\"\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_question},\n",
    "        ],\n",
    "        response_format=QuestionType,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Cell 7: Get schema string\n",
    "# The function get_schema_string retrieves the schema information from the Memgraph database and formats it into a human-readable string.\n",
    "# Getting the full schema string from the database\n",
    "def get_schema_string(db_client) -> str:\n",
    "    \n",
    "    with db_client.session() as session:\n",
    "        schema = session.run(\"SHOW SCHEMA INFO\")\n",
    "        schema_info = json.loads(schema.single().value())\n",
    "        nodes = schema_info[\"nodes\"]\n",
    "        edges = schema_info[\"edges\"]\n",
    "        node_indexes = schema_info[\"node_indexes\"]\n",
    "        edge_indexes = schema_info[\"edge_indexes\"]\n",
    "\n",
    "        schema_str = \"Nodes:\\n\"\n",
    "        for node in nodes:\n",
    "            properties = \", \".join(\n",
    "                f\"{prop['key']}: {', '.join(t['type'] for t in prop['types'])}\"\n",
    "                for prop in node[\"properties\"]\n",
    "            )\n",
    "            schema_str += f\"Labels: {node['labels']} | Properties: {properties}\\n\"\n",
    "\n",
    "        schema_str += \"\\nEdges:\\n\"\n",
    "        for edge in edges:\n",
    "            properties = \", \".join(\n",
    "                f\"{prop['key']}: {', '.join(t['type'] for t in prop['types'])}\"\n",
    "                for prop in edge[\"properties\"]\n",
    "            )\n",
    "            schema_str += f\"Type: {edge['type']} | Start Node Labels: {edge['start_node_labels']} | End Node Labels: {edge['end_node_labels']} | Properties: {properties}\\n\"\n",
    "\n",
    "        schema_str += \"\\nNode Indexes:\\n\"\n",
    "        for index in node_indexes:\n",
    "            schema_str += (\n",
    "                f\"Labels: {index['labels']} | Properties: {index['properties']}\\n\"\n",
    "            )\n",
    "\n",
    "        schema_str += \"\\nEdge Indexes:\\n\"\n",
    "        for index in edge_indexes:\n",
    "            schema_str += f\"Type: {index['type']} | Properties: {index['properties']}\\n\"\n",
    "\n",
    "        return schema_str\n",
    "\n",
    "# Cell 8: Text to Cypher\n",
    "# The function text_to_Cypher translates a natural language question into a Cypher query using the OpenAI API.\n",
    "# It leverages the database schema to generate accurate queries and includes error correction and retry logic.\n",
    "# Tool used to run text_to_Cypher\n",
    "def text_to_Cypher(db_client, openai_client, user_question) -> Dict:\n",
    "    logger.info(\"Running text_to_cypher tool\")\n",
    "\n",
    "    schema = get_schema_string(db_client)\n",
    "    prompt_user = f\"\"\"\n",
    "\n",
    "    User Question: \"{user_question}\"\n",
    "    Schema: {schema}\n",
    "\n",
    "    Based on schema and question, generate a Cypher query that directly corresponds to the user's intent.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_developer = f\"\"\"\n",
    "    Your task is to directly translate natural language\n",
    "    inquiry into precise and executable Cypher query for Memgraph database.\n",
    "    You will utilize a provided database schema to understand the structure,\n",
    "    nodes and relationships within the Memgraph database.\n",
    "\n",
    "    Rules:\n",
    "    - Use provided node and relationship labels and property names from the\n",
    "    schema which describes the database's structure. Upon receiving a user question, synthesize the\n",
    "    schema to craft a precise Cypher query that directly corresponds to\n",
    "    the user's intent.\n",
    "    - Generate valid executable Cypher queries on top of Memgraph database.\n",
    "    - Use Memgraph MAGE procedures instead of Neo4j APOC procedures.\n",
    "\n",
    "    With all the above information and instructions, generate Cypher query\n",
    "    for the user question.\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count_user = len(encoding.encode(prompt_user))\n",
    "    token_count_developer = len(encoding.encode(prompt_developer))\n",
    "    token_count = token_count_user + token_count_developer\n",
    "    logger.info(f\"Token count on prompt : {token_count}\")\n",
    "\n",
    "    prompt_chain = [\n",
    "            {\"role\": \"developer\", \"content\": prompt_developer},\n",
    "            {\"role\": \"user\", \"content\": prompt_user},\n",
    "    ]\n",
    "\n",
    "    tool_response = ToolResponse()\n",
    "\n",
    "    query = \"\"\n",
    "    if token_count <= MODEL[\"context_window\"]:\n",
    "        query = generate_cypher_query(openai_client, prompt_chain)\n",
    "    else:\n",
    "        return tool_response.set_status(False).set_results(\"Token count exceeded the limit.\")\n",
    "\n",
    "    logger.info(\"### Cypher Query:\")\n",
    "    logger.info(query)\n",
    "    \n",
    "    res = []\n",
    "    with db_client.session() as session:\n",
    "        for _ in range(3):  # Try correction process up to 3 times\n",
    "            try:\n",
    "                results = session.run(query)\n",
    "                if not results.peek():\n",
    "                    raise ValueError(\n",
    "                        \"The query did not return any results. There is a possible issue with the query \"\n",
    "                        \"labels and parameters, if you are matching strings consider matching them in the case-insensitive way.\"\n",
    "                    )\n",
    "                for record in results:\n",
    "                    res.append(record)\n",
    "\n",
    "                return tool_response.set_status(True).set_results(res)\n",
    "\n",
    "            except (ValueError, Exception) as e:\n",
    "                error_type = \"ValueError\" if isinstance(e, ValueError) else \"Error\"\n",
    "                logger.error(f\"{error_type} in running the query:\")\n",
    "                logger.error(e)\n",
    "                error_message = str(e)\n",
    "\n",
    "                prompt_correction = f\"\"\"\n",
    "                The following Cypher query generated a {error_type}:\n",
    "                Query: {query}\n",
    "                Error: {error_message}\n",
    "                Question: {user_question}\n",
    "\n",
    "                Please correct the Cypher query based on the error, schema and question.\n",
    "                \"\"\"\n",
    "                prompt_chain.append({\"role\": \"assistant\", \"content\": query})\n",
    "                prompt_chain.append({\"role\": \"developer\", \"content\": prompt_correction})\n",
    "\n",
    "                query = generate_cypher_query(openai_client, prompt_chain)\n",
    "                logger.info(\"### Corrected Cypher Query:\")\n",
    "                logger.info(query)\n",
    "\n",
    "        return tool_response.set_status(False).set_results(\"Error in running the query.\")\n",
    "\n",
    "# Cell 9: Schema Tool\n",
    "# Schema tool\n",
    "def schema_tool(db_client) -> ToolResponse:\n",
    "    return ToolResponse(True, get_schema_string(db_client))\n",
    "\n",
    "\n",
    "\n",
    "# Cell 10: Generate Cypher Query\n",
    "# The function generate_cypher_query is a helper function to generate a Cypher query using the OpenAI API.\n",
    "# It takes the OpenAI client and prompt messages as input and returns the generated Cypher query string.\n",
    "# Agent generation of a Cypher question\n",
    "def generate_cypher_query(openai_client, prompt_messages):\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=prompt_messages,\n",
    "        response_format=CypherQuery,\n",
    "    )\n",
    "    return completion.choices[0].message.parsed.query\n",
    "\n",
    "\n",
    "# Cell 11: Config Tool\n",
    "# Config tool used to retrive the configuration from the database \n",
    "def config_tool(db_client) -> ToolResponse:\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            config = session.run(\"SHOW CONFIG\")\n",
    "            config_str = \"Configurations:\\n\"\n",
    "            for record in config:\n",
    "                config_str += f\"Name: {record['name']} | Default Value: {record['default_value']} | Current Value: {record['current_value']} | Description: {record['description']}\\n\"\n",
    "            return ToolResponse(True, config_str)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the Config tool query.\")\n",
    "        return ToolResponse(False, \"Error in running the Config tool query.\")\n",
    "\n",
    "\n",
    "# Cell 12: PageRank Choice\n",
    "# Agent page rank choice\n",
    "def page_rank_choice(openai_client, user_question) -> Dict:\n",
    "    question = f\"<Question>{user_question}</Question>\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the provided question, try to guess how many nodes should be returned from the PageRank in the assesment. \n",
    "    The question is in <Question> </Question> format.\n",
    "    \"\"\"\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        response_format=PageRankNodes,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Cell 13: PageRank Tool\n",
    "\n",
    "# Page Rank tool\n",
    "def page_rank_tool(db_client, openai_client, user_question) -> ToolResponse:\n",
    "\n",
    "    prompt_developer = f\"\"\"\n",
    "    Based on the provided question, try to guess how many nodes should be returned from the PageRank in the assesment. \n",
    "    The question is in <Question> </Question> format.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": prompt_developer},\n",
    "        {\"role\": \"user\", \"content\": \"<Question>\" + user_question + \"</Question>\"},\n",
    "    ]\n",
    "\n",
    "    choice = page_rank_choice(openai_client, user_question)\n",
    "\n",
    "    logger.info(\"Running the PageRank tool\")\n",
    "    logger.info(f\"Number of nodes: {choice.number_of_nodes}\")\n",
    "\n",
    "    with db_client.session() as session:\n",
    "        try:\n",
    "            result = session.run(f\"CALL pagerank.get() YIELD node, rank RETURN node, rank LIMIT {choice.number_of_nodes};\")\n",
    "            result_str = \"\"\n",
    "            for record in result:\n",
    "                node = record[\"node\"]\n",
    "                properties = {k: v for k, v in node.items() if k != \"embedding\"}\n",
    "                result_str += f\"Node: {properties}, Rank: {record['rank']}\\n\"\n",
    "            \n",
    "            logger.info(\"Page rank successful\") \n",
    "            logger.info(result_str)\n",
    "            return ToolResponse(True, result_str)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in running the PageRank tool query.\")\n",
    "            return ToolResponse(False, \"Error in running the PageRank tool query.\")\n",
    "\n",
    "# Cell 14: Community tool\n",
    "def community_tool(db_client) -> ToolResponse:\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            result = session.run(\"MATCH (n:Community) RETURN n.id, n.summary;\")\n",
    "            result_str = \"\"\n",
    "            for record in result:\n",
    "                result_str += f\"Community ID: {record['n.id']}, Summary: {record['n.summary']}\\n\"\n",
    "            return ToolResponse(True, result_str)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the Community tool query.\")\n",
    "        return ToolResponse(False, \"Error in running the Community tool query.\")\n",
    "\n",
    "\n",
    "def community_prompt(openai_client, community_string) -> Dict:\n",
    "    prompt = f\"Summarize the following community information into 5 to 10 sentences, you will get the community string in the <Community> </Community> format\"\n",
    "    prompt_community= f\"<Community>{community_string}</Community>\"\n",
    "\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_community},\n",
    "        ],\n",
    "        response_format=Community,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "def precompute_community_summary(db_client, openai_client) -> Dict:\n",
    "\n",
    "    number_of_communities = 0\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            CALL community_detection.get()\n",
    "            YIELD node, community_id \n",
    "            SET node.community_id = community_id;\n",
    "            \"\"\"\n",
    "            )\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            RETURN count(distinct n.community_id) as community_count;\n",
    "            \"\"\"\n",
    "            )\n",
    "            for record in result:\n",
    "                number_of_communities = record['community_count']\n",
    "                print(f\"Number of communities: {record['community_count']}\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the community detection query.\")\n",
    "        return False; \n",
    "    \n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            communities = []\n",
    "            for i in range(0, number_of_communities):\n",
    "                community_string = \"\"\n",
    "                community_id = 0\n",
    "                result = session.run(f\"\"\"\n",
    "                MATCH (start), (end) \n",
    "                WHERE start.community_id = {i} AND end.community_id = {i} AND id(start) < id(end)\n",
    "                MATCH p = (start)-[*..1]-(end)\n",
    "                RETURN p; \n",
    "                \"\"\")\n",
    "                for record in result:\n",
    "                    path = record['p']\n",
    "                    for rel in path.relationships:\n",
    "                        start_node = rel.start_node\n",
    "                        end_node = rel.end_node\n",
    "                        start_node_properties = {k: v for k, v in start_node.items() if k != 'embedding'}\n",
    "                        end_node_properties = {k: v for k, v in end_node.items() if k != 'embedding'}\n",
    "                        community_string += f\"({start_node_properties})-[:{rel.type}]->({end_node_properties})\\n\"\n",
    "                        community_id = i\n",
    "                communities.append({\"id\": community_id, \"data\": community_string})\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the community detection query.\")\n",
    "        return False;\n",
    "        \n",
    "    logger.info(\"Total number of communities:\")\n",
    "    logger.info(number_of_communities)\n",
    "    community_summary = []\n",
    "    for community in communities:\n",
    "        community_id = community['id']\n",
    "        community_string = community['data']\n",
    "        try:\n",
    "            logging.info(f\"Generating summary for community {community_id}\")\n",
    "            prompt = community_prompt(openai_client, community_string)\n",
    "            community_summary.append({\"id\": community_id, \"summary\": prompt.summary})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in generating summary for community {community_id} and community string {community_string}\")\n",
    "            return False;\n",
    "\n",
    "    try:\n",
    "        with db_client.session() as session:\n",
    "            for community in community_summary:\n",
    "                community_id = community['id']\n",
    "                summary = community['summary']\n",
    "                session.run(\n",
    "                    \"CREATE (c:Community { id: $id, summary: $summary})\",\n",
    "                    summary=summary, \n",
    "                    id=community_id\n",
    "                )\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in running the community detection query.\")\n",
    "        return False;\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Cell 15 - Vector Relevance Expansion\n",
    "\n",
    "def decide_on_structure_parameters(openai_client, messages) -> Dict:\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=messages,\n",
    "        response_format=StructureQuestionData,\n",
    "    )\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "\n",
    "def vector_relevance_expansion(db_client, openai_client, user_question) -> Dict:\n",
    "\n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    question_embedding = model.encode(user_question)\n",
    "\n",
    "    prompt_parameters = f\"\"\"\n",
    "    You will get a question about the structure of the graph. The vector search\n",
    "    will find the most similar node based on the question embedding an node\n",
    "    embedding, and then return the data connected to the most similar nodes that\n",
    "    are hops away. Your task is to find out how many nodes should vector search\n",
    "    return and how many hops should be used to find the relevant data. If the\n",
    "    question is about undefined number of node guess the intended number of\n",
    "    nodes, by default consider 1. If the question is about undefined number of\n",
    "    hops guess the intended number of hops, by default consider 1.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [ \n",
    "        {\"role\": \"developer\", \"content\": prompt_parameters}, \n",
    "        {\"role\": \"user\", \"content\": user_question} \n",
    "        ]\n",
    "\n",
    "    structure_parameters = decide_on_structure_parameters(openai_client, messages)\n",
    "\n",
    "    logger.info(\"Structure parameters:\")\n",
    "    logger.info(structure_parameters)\n",
    "\n",
    "    nodes = find_most_similar_nodes(db_client, user_question,  question_embedding, structure_parameters.number_of_similar_nodes)\n",
    "\n",
    "\n",
    "    for node in nodes:\n",
    "        logger.info(\"Most similar nodes:\")\n",
    "        logger.info(node)\n",
    "\n",
    "    tool_response = ToolResponse()\n",
    "    if nodes is None:\n",
    "        return tool_response.set_status(False).set_results(\"No similar nodes found.\")\n",
    "\n",
    "    relevant_data = get_relevant_data(db_client, nodes, structure_parameters.number_of_hops)\n",
    "\n",
    "    return tool_response.set_status(True).set_results(relevant_data)\n",
    "    \n",
    "\n",
    "def find_most_similar_nodes(db_client, user_question,  question_embedding, number_of_similar_nodes):\n",
    "        \n",
    "    with db_client.session() as session:\n",
    "        result = session.run(\n",
    "            f\"CALL vector_search.search('index_name', {number_of_similar_nodes}, {question_embedding.tolist()}) YIELD * RETURN *;\"\n",
    "        )\n",
    "        nodes_data = []\n",
    "        for record in result:\n",
    "            node = record[\"node\"]\n",
    "            properties = {k: v for k, v in node.items() if k != \"embedding\"}\n",
    "            node_data = {\n",
    "                \"distance\": record[\"distance\"],\n",
    "                \"id\": node.element_id,\n",
    "                \"labels\": list(node.labels),\n",
    "                \"properties\": properties,\n",
    "            }\n",
    "\n",
    "            nodes_data.append(node_data)\n",
    "        print(\"All similar nodes:\")\n",
    "        for node in nodes_data:\n",
    "            print(node)\n",
    "\n",
    "        return nodes_data if nodes_data else None\n",
    "\n",
    "\n",
    "def get_relevant_data(db_client, nodes, hops):\n",
    "    paths = []\n",
    "    for node in nodes:\n",
    "        with db_client.session() as session:\n",
    "            query = (\n",
    "                f\"MATCH path=((n)-[r*..{hops}]-(m)) WHERE id(n) = {node['id']} RETURN path\"\n",
    "            )\n",
    "            result = session.run(query)\n",
    "            \n",
    "            for record in result:\n",
    "                path_data = []\n",
    "                for segment in record[\"path\"]:\n",
    "\n",
    "                    # Process start node without 'embedding' property\n",
    "                    start_node_data = {\n",
    "                        k: v for k, v in segment.start_node.items() if k != \"embedding\"\n",
    "                    }\n",
    "\n",
    "                    # Process relationship data\n",
    "                    relationship_data = {\n",
    "                        \"type\": segment.type,\n",
    "                        \"properties\": segment.get(\"properties\", {}),\n",
    "                    }\n",
    "\n",
    "                    # Process end node without 'embedding' property\n",
    "                    end_node_data = {\n",
    "                        k: v for k, v in segment.end_node.items() if k != \"embedding\"\n",
    "                    }\n",
    "\n",
    "                    # Add to path_data as a tuple (start_node, relationship, end_node)\n",
    "                    path_data.append((start_node_data, relationship_data, end_node_data))\n",
    "\n",
    "                paths.append(path_data)\n",
    "\n",
    "    return paths\n",
    "\n",
    "# Cell 16 - Generate final response \n",
    "\n",
    "def generate_final_response(openai_client, results, user_question: str):\n",
    "    prompt = f\"\"\"\n",
    "    Using the data and the user's original question, generate a final answer:\n",
    "    User Question: \"{user_question}\"\n",
    "    Data from the database: {results}\n",
    "\n",
    "    Try to answer the user's question using just the the provided data..\n",
    "    \n",
    "    \"\"\"\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message\n",
    "\n",
    "\n",
    "\n",
    "# Cell 17 - Tool selection pipe\n",
    "\n",
    "\n",
    "def tool_selection_pipe(openai_client, user_question, question_type) -> Dict:\n",
    "    question = f\"<Question>{user_question}</Question>\"\n",
    "    question_type = f\"<Type>{question_type}</Type>\"\n",
    "    prompt_developer = f\"\"\"\n",
    "\n",
    "    Based on the question type and the user's question, select the most appropriate tool option and second option as a backup to answer the question:\n",
    "\n",
    "    Retrival - direct lookups, specific and well-defined. The query seeks information about specific entities (nodes or relationships).\n",
    "    Options: \n",
    "        - Cypher: A tool that generates a Cypher query based on the user's question and the database schema.\n",
    "        - Vector Relevance Expansion: A tool that finds the most similar nodes based on the user's question and the database schema.\n",
    "    Structure - exploratory, the query seeks information about the structure of the graph, close relationships between entities, or properties of nodes.\n",
    "    Options:\n",
    "        - Cypher: A tool that generates a Cypher query based on the user's question and the database schema.\n",
    "        - Vector Relevance Expansion: A tool that finds the most similar nodes based on the user's question and the database schema.\n",
    "        - Global - the query seeks context about the entire graph, community, such as the most important node or global trends in graph.\n",
    "    Global - the query seeks context about the entire graph, community, such as the most important node or global trends in graph.\n",
    "    Options:\n",
    "        - PageRank: A tool that provides PageRank information about the graph and its nodes, it can help with identifying the most important nodes.\n",
    "        - Community: A tool that provides communities information about the graph, it contains the summary of the community, and can help with global insights.\n",
    "    Database - the query seeks statistical information about the database, such as index information, node count, or relationship count, config etc.\n",
    "    Options:\n",
    "        - Schema: A tool that provides schema information about the dataset and datatypes.\n",
    "        - Config: A tool that provides configuration information about the database.\n",
    "        - Cypher: A tool that generates a Cypher query based on the user's question and the database schema.\n",
    "\n",
    "    The question is in <Question> </Question> format, and the type of the question is <Type> </Type>.\n",
    "\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": prompt_developer},\n",
    "        {\"role\": \"user\", \"content\": question + question_type},\n",
    "\n",
    "    ]\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL[\"name\"],\n",
    "        messages=messages,\n",
    "        response_format=ToolSelection,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "\n",
    "# Cell 18 - Tool execution pipe \n",
    "\n",
    "def tool_execution(tool: str, db_client, openai_client, user_question) -> ToolResponse:\n",
    "    \"\"\"\n",
    "    Simulates execution success/failure for demo purposes.\n",
    "    Replace this with actual logic to execute each tool.\n",
    "    \"\"\"\n",
    "    # For demo, let's assume \"Text to Cypher\" always succeeds\n",
    "    if tool == \"Cypher\":\n",
    "        return text_to_Cypher(db_client, openai_client, user_question)\n",
    "    elif tool == \"Vector Relevance Expansion\":\n",
    "        return vector_relevance_expansion(db_client, openai_client, user_question)\n",
    "    elif tool == \"PageRank\":\n",
    "        return page_rank_tool(db_client, openai_client, user_question)\n",
    "    elif tool == \"Community\":\n",
    "        return community_tool(db_client)\n",
    "    elif tool == \"Schema\":\n",
    "        return  schema_tool(db_client)\n",
    "    elif tool == \"Config\":\n",
    "        return config_tool(db_client)\n",
    "    else:\n",
    "        return ToolResponse(False, \"Tool execution failed, tool not found.\")\n",
    "\n",
    "\n",
    "def execute_tool(tool: str, user_question: str, db_client,  openai_client ) -> ToolResponse:\n",
    "    \"\"\"\n",
    "    Executes the given tool based on its name.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    response = None\n",
    "    try:\n",
    "        logger.info(f\"Trying tool: {tool}\")\n",
    "        response = tool_execution(tool, db_client, openai_client, user_question)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing {tool}: {e}\")\n",
    "        return response\n",
    "\n",
    "\n",
    "# Cell 19 - Prep and running \n",
    "\n",
    "def index_setup(db_client):\n",
    "    with db_client.session() as session:\n",
    "        print(\"Creating the vector index...\")\n",
    "        session.run(\n",
    "            \"\"\"\n",
    "            CREATE VECTOR INDEX index_name ON :Entity(embedding) WITH CONFIG {\"dimension\": 384, \"capacity\": 2000, \"metric\": \"cos\",\"resize_coefficient\": 2};\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "def compute_node_embeddings(db_client):\n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    with db_client.session() as session:\n",
    "        # Retrieve all nodes\n",
    "        result = session.run(\"MATCH (n) RETURN n\")\n",
    "        print(\"Embedded data: \")\n",
    "        for record in result:\n",
    "            node = record[\"n\"]\n",
    "            # Check if the node already has an embedding\n",
    "            if \"embedding\" in node:\n",
    "                print(\"Embedding already exists\")\n",
    "                return\n",
    "\n",
    "            # Combine node labels and properties into a single string\n",
    "            node_data = (\n",
    "                \" \".join(node.labels)\n",
    "                + \" \"\n",
    "                + \" \".join(f\"{k}: {v}\" for k, v in node.items())\n",
    "            )\n",
    "            print(node_data)\n",
    "            # Compute the embedding for the node\n",
    "            node_embedding = model.encode(node_data)\n",
    "\n",
    "            # Store the embedding back into the node\n",
    "            session.run(\n",
    "                f\"MATCH (n) WHERE id(n) = {node.element_id} SET n.embedding = {node_embedding.tolist()}\"\n",
    "            )\n",
    "\n",
    "        session.run(\"MATCH (n) SET n:Entity\")\n",
    "\n",
    "\n",
    "@st.cache_resource()\n",
    "def get_openai_client():\n",
    "    return OpenAI()\n",
    "\n",
    "@st.cache_resource()\n",
    "def get_db_client():\n",
    "    return neo4j.GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"\", \"\"))\n",
    "\n",
    "@st.cache_resource()\n",
    "def preprocess_data(_db_client, _openai_client):\n",
    "    # status = precompute_community_summary(db_client, openai_client)\n",
    "    # if status:\n",
    "    #     logger.info(\"Community summary precomputed.\")\n",
    "    # else:\n",
    "    #     logger.error(\"Error in precomputing community summary.\")\n",
    "    #     logger.error(\"Community questions will fail\")\n",
    "\n",
    "    index_setup(db_client)\n",
    "    compute_node_embeddings(db_client)\n",
    "    return \"Proccessing data completed\"\n",
    "\n",
    "\n",
    "def main(db_client, openai_client):\n",
    "\n",
    "    st.title(\"Agentic GraphRAG with Memgraph\")\n",
    "\n",
    "    # User input\n",
    "    user_question = st.text_input(\"Enter your question about the dataset:\", \"\")\n",
    "\n",
    "    if st.button(\"Run GraphRAG Pipeline\"):\n",
    "        if user_question.strip():\n",
    "\n",
    "            st.write(\"## Classifying Question type...\")\n",
    "            logger.info(\"Classifying Question type...\")\n",
    "            question_type = classify_the_question(openai_client, user_question)\n",
    "\n",
    "            st.write(\"### Question Type:\")\n",
    "            st.write(\"*Type*: \", question_type.type)\n",
    "            st.write(\"*Explanation*: \", question_type.explanation)\n",
    "\n",
    "            st.write(\"## Running the tool selection...\")\n",
    "            \n",
    "            tools = tool_selection_pipe(openai_client, user_question, question_type)\n",
    "\n",
    "            st.write(\"### Tools selected:\") \n",
    "            st.write(\"Tool 1: \", tools.first_pick)  \n",
    "            st.write(\"Tool 2: \", tools.second_pick)\n",
    "\n",
    "             # Try first pick\n",
    "            response = execute_tool(tools.first_pick, user_question, db_client, openai_client)\n",
    "            if response.status:\n",
    "                logger.info(f\"First pick: '{tools.first_pick}' succeeded.\")\n",
    "                st.write(f\"#### Tool 1: '{tools.first_pick}' has succeeded.\")\n",
    "            else:\n",
    "                st.write(f\"Tool 1:'{tools.first_pick}' has failed.\")\n",
    "                response = execute_tool(tools.second_pick, user_question, db_client, openai_client)\n",
    "                if response.status:\n",
    "                    st.write(f\"#### Tool 2: '{tools.second_pick}' has succeeded.\")\n",
    "\n",
    "            st.write(\"### Tool Execution Completed.\")\n",
    "\n",
    "            if response.status is False:\n",
    "                st.error(\"Tool execution has failed.\")\n",
    "            else:\n",
    "                st.write(\"### Tool Response:\")\n",
    "                formated_response = {\"Response\": response.results}\n",
    "                st.json(formated_response)\n",
    "\n",
    "                st.write(\"## Generating Final Response...\")\n",
    "\n",
    "                final_response = generate_final_response(\n",
    "                    openai_client, response.results, user_question\n",
    "                )\n",
    "                st.write(\"### Final Response:\")\n",
    "                st.write(final_response.content)\n",
    "                st.write(\"## Agentic GraphRAG Pipeline Completed.\")\n",
    "                \n",
    "        else:\n",
    "            st.error(\"Please enter a question to proceed.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    openai_client = get_openai_client()\n",
    "\n",
    "    db_client = get_db_client()\n",
    "\n",
    "    preprocess_data(db_client, openai_client)\n",
    "\n",
    "    main(db_client, openai_client)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
